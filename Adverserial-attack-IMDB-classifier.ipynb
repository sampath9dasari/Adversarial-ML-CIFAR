{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Reshape, Activation, Input\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences, with shape (25000,)\n",
      "25000 test sequences with shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "max_features = 10000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=1)\n",
    "print(len(x_train), 'train sequences, with shape', x_train.shape)\n",
    "print(len(x_test), 'test sequences with shape', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decreasing the size of test set.\n",
    "np.random.seed(10)\n",
    "idx = np.random.choice(x_test.shape[0],2000)\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data review statistics:\n",
      "count    25000.000000\n",
      "mean       238.713640\n",
      "std        176.497204\n",
      "min         11.000000\n",
      "25%        130.000000\n",
      "50%        178.000000\n",
      "75%        291.000000\n",
      "max       2494.000000\n",
      "dtype: float64\n",
      "\n",
      "Test data review statistics:\n",
      "count    2000.000000\n",
      "mean      229.910000\n",
      "std       167.108499\n",
      "min        29.000000\n",
      "25%       127.000000\n",
      "50%       174.000000\n",
      "75%       284.000000\n",
      "max      1158.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000, 2)\n",
      "test labels shape: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 150)\n",
      "test data shape: (2000, 150)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 150\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 512\n",
    "embedding_size = 128\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 8\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordnum = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(wordnum)\n",
    "lstm_out = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,411,842\n",
      "Trainable params: 1,411,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_clf = Model(inputs=wordnum, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 378s 15ms/sample - loss: 0.6619 - accuracy: 0.5884 - val_loss: 0.6190 - val_accuracy: 0.6320\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 371s 15ms/sample - loss: 0.5938 - accuracy: 0.6941 - val_loss: 0.5702 - val_accuracy: 0.7015\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 372s 15ms/sample - loss: 0.5726 - accuracy: 0.7124 - val_loss: 0.5112 - val_accuracy: 0.7480\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 420s 17ms/sample - loss: 0.4264 - accuracy: 0.8174 - val_loss: 0.3563 - val_accuracy: 0.8490\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 423s 17ms/sample - loss: 0.3126 - accuracy: 0.8735 - val_loss: 0.3266 - val_accuracy: 0.8585\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 434s 17ms/sample - loss: 0.2399 - accuracy: 0.9080 - val_loss: 0.3320 - val_accuracy: 0.8560\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 445s 18ms/sample - loss: 0.1842 - accuracy: 0.9317 - val_loss: 0.3585 - val_accuracy: 0.8540\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 431s 17ms/sample - loss: 0.1447 - accuracy: 0.9478 - val_loss: 0.4088 - val_accuracy: 0.8550\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 428s 17ms/sample - loss: 0.1035 - accuracy: 0.9668 - val_loss: 0.5002 - val_accuracy: 0.8425\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 439s 18ms/sample - loss: 0.0783 - accuracy: 0.9751 - val_loss: 0.6003 - val_accuracy: 0.8400\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 413s 17ms/sample - loss: 0.0576 - accuracy: 0.9829 - val_loss: 0.6739 - val_accuracy: 0.8325\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 415s 17ms/sample - loss: 0.0442 - accuracy: 0.9878 - val_loss: 0.6872 - val_accuracy: 0.8405\n"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train,\n",
    "                             validation_data=(x_test, y_test),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=64,\n",
    "                             epochs=12\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate over Test data:\n",
      "2000/2000 [==============================] - 10s 5ms/sample - loss: 0.6872 - accuracy: 0.8405\n",
      "Loss over Test data: 0.6872255901098251\n",
      "Accuracy over Test data: 0.8405\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate over Test data:\")\n",
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test)\n",
    "print('Loss over Test data:', loss)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (10000, 128)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (150, 128)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf.save(\"imdb_compiled_clf_150dim.h5\")\n",
    "imdb_clf.save_weights(\"imdb_model_weights_150dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sub-model - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150, 128)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 131,842\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    # if not compare_losses(x, y, pred_y) : return None\n",
    "    return x_gradients\n",
    "\n",
    "def compare_losses(x, labels, preds):\n",
    "    # Calculate loss\n",
    "    calc_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=preds)\n",
    "    model_loss, _ = imdb_clf.evaluate(x.reshape(-1,maxlen),labels.reshape(-1,num_classes))\n",
    "\n",
    "    return calc_loss-model_loss<0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "\n",
    "    _ , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "\n",
    "    for word in range(maxlen):\n",
    "\n",
    "        if acc<1.0 : break\n",
    "\n",
    "        word_grad = x_gradient[np.argmax(y), word]\n",
    "        # print(word_grad.shape)\n",
    "\n",
    "        jac_sign = np.sign(word_grad).sum()\n",
    "        vocab_sign = np.add.reduce(np.sign(word_grad - vocab_embeddings),1)\n",
    "#         jac_sign = np.sign(word_grad)\n",
    "#         vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "\n",
    "        match_word = np.argmin(np.absolute(vocab_sign - jac_sign))\n",
    "#         match_word = np.argmin(np.absolute(np.add.reduce(vocab_sign - jac_sign, axis=1)))\n",
    "        x[word] = match_word\n",
    "\n",
    "        loss , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "        \n",
    "#     print(word,acc)\n",
    "    if acc<1.0:\n",
    "        return x, word\n",
    "    else:\n",
    "        return  x_copy, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "# idx = np.random.choice(x_train.shape[0], 10)\n",
    "idx = range(512)\n",
    "xs, ys = x_train[idx].copy(), y_train[idx].copy()\n",
    "# xs, ys = x_train.copy(), y_train.copy()\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "print(\"Loss and accuracy of selected samples:\", imdb_clf.evaluate(xs, ys, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting adversarial samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/25000 [00:11<76:48:53, 11.06s/it]\u001b[A\n",
      "  0%|          | 2/25000 [00:40<114:30:09, 16.49s/it]\u001b[A\n",
      "  0%|          | 3/25000 [00:59<120:59:17, 17.42s/it]\u001b[A\n",
      "  0%|          | 4/25000 [01:20<128:26:58, 18.50s/it]\u001b[A\n",
      "  0%|          | 5/25000 [01:47<145:48:10, 21.00s/it]\u001b[A\n",
      "  0%|          | 6/25000 [02:14<157:42:41, 22.72s/it]\u001b[A\n",
      "  0%|          | 7/25000 [02:36<157:19:08, 22.66s/it]\u001b[A\n",
      "  0%|          | 8/25000 [02:39<115:47:45, 16.68s/it]\u001b[A\n",
      "  0%|          | 9/25000 [03:08<141:25:51, 20.37s/it]\u001b[A\n",
      "  0%|          | 10/25000 [03:35<149:52:35, 21.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 69.7\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f84d9e9cb70>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py\", line 4167, in <genexpr>\n",
      "    ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 10 input samples and 25000 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-f5950f5ea2f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average number of changes per sample:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mimdb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrafted_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2483\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m         \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    742\u001b[0m                      \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                      \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    745\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 10 input samples and 25000 target samples."
     ]
    }
   ],
   "source": [
    "print(\"Crafting adversarial samples...\")\n",
    "for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "    # x = x_train[idx]\n",
    "    # y = y_train[idx]\n",
    "    new_x , changes = craft_sample(x, y, grad)\n",
    "    crafted_x.append(new_x)\n",
    "    num_changes.append(changes)\n",
    "\n",
    "crafted_x = np.array(crafted_x)\n",
    "num_changes = np.array(num_changes)\n",
    "\n",
    "print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "imdb_clf.evaluate(crafted_x, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Crafted_x_train_150dim.pickle', 'ab') as fo:     \n",
    "    pickle.dump(crafted_x, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('crafted_x_train_150dim.csv', crafted_x, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
