{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Reshape, Activation, Input\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences, with shape (25000,)\n",
      "25000 test sequences with shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "max_features = 5000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=1)\n",
    "print(len(x_train), 'train sequences, with shape', x_train.shape)\n",
    "print(len(x_test), 'test sequences with shape', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data review statistics:\n",
      "count    25000.000000\n",
      "mean       238.713640\n",
      "std        176.497204\n",
      "min         11.000000\n",
      "25%        130.000000\n",
      "50%        178.000000\n",
      "75%        291.000000\n",
      "max       2494.000000\n",
      "dtype: float64\n",
      "\n",
      "Test data review statistics:\n",
      "count    25000.000000\n",
      "mean       230.804200\n",
      "std        169.164471\n",
      "min          7.000000\n",
      "25%        128.000000\n",
      "50%        174.000000\n",
      "75%        280.000000\n",
      "max       2315.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000, 2)\n",
      "test labels shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 80)\n",
      "test data shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 80\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 256\n",
    "embedding_size = 128\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 18\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordnum = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(wordnum)\n",
    "lstm_out = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 80)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 80, 128)           640000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 771,842\n",
      "Trainable params: 771,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_clf = Model(inputs=wordnum, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/18\n",
      "25000/25000 [==============================] - 97s 4ms/sample - loss: 0.5841 - accuracy: 0.6858 - val_loss: 0.4358 - val_accuracy: 0.8114\n",
      "Epoch 2/18\n",
      "25000/25000 [==============================] - 91s 4ms/sample - loss: 0.4228 - accuracy: 0.8189 - val_loss: 0.4093 - val_accuracy: 0.8152\n",
      "Epoch 3/18\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.3826 - accuracy: 0.8411 - val_loss: 0.4193 - val_accuracy: 0.8145\n",
      "Epoch 4/18\n",
      "18944/25000 [=====================>........] - ETA: 17s - loss: 0.3579 - accuracy: 0.8540"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train,\n",
    "                             validation_data=(x_test, y_test),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate over Test data:\")\n",
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test)\n",
    "print('Loss over Test data:', loss)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (5000, 128)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (80, 128)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sub-model - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 80, 128)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 131,842\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(x_gradients.shape)\n",
    "\n",
    "    # if not compare_losses(x, y, pred_y) : return None\n",
    "    return x_gradients\n",
    "\n",
    "def compare_losses(x, labels, preds):\n",
    "    # Calculate loss\n",
    "    calc_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=preds)\n",
    "    model_loss, _ = imdb_clf.evaluate(x.reshape(-1,maxlen),labels.reshape(-1,num_classes))\n",
    "\n",
    "    return calc_loss-model_loss<0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "\n",
    "    _ , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "\n",
    "    for word in range(maxlen):\n",
    "\n",
    "        if acc<1.0 : break\n",
    "\n",
    "        word_grad = x_gradient[np.argmax(y), word]\n",
    "        # print(word_grad.shape)\n",
    "\n",
    "#         jac_sign = np.sign(word_grad).sum()\n",
    "#         vocab_sign = np.add.reduce(np.sign(word_grad - vocab_embeddings),1)\n",
    "        jac_sign = np.sign(word_grad)\n",
    "        vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "\n",
    "        match_word = np.argmin(np.sum(vocab_sign - jac_sign))\n",
    "        x[word] = match_word\n",
    "\n",
    "        loss , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "        \n",
    "    if acc<1.0:\n",
    "        return x, word\n",
    "    else:\n",
    "        return  x_copy, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2, 80, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 2/10 [00:04<00:19,  2.43s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:08<00:20,  2.94s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:15<00:24,  4.05s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:22<00:24,  4.90s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "idx = np.random.choice(x_train.shape[0], 10)\n",
    "xs, ys = x_train[idx], y_train[idx]\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "# print(imdb_clf.evaluate(xs, ys))\n",
    "\n",
    "for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "    # x = x_train[idx]\n",
    "    # y = y_train[idx]\n",
    "    new_x , changes = craft_sample(x, y, grad)\n",
    "    crafted_x.append(new_x)\n",
    "    num_changes.append(changes)\n",
    "\n",
    "crafted_x = np.array(crafted_x)\n",
    "num_changes = np.array(num_changes)\n",
    "\n",
    "print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "imdb_clf.evaluate(crafted_x, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "(10, 2, 80, 128)\n",
      "Crafting adversarial samples...\n",
      "CPU times: user 43.8 s, sys: 872 ms, total: 44.7 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "idx = np.random.choice(x_train.shape[0], 10)\n",
    "xs, ys = x_train[idx], y_train[idx]\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "print(\"Crafting adversarial samples...\")\n",
    "craft_pool = mp.Pool(mp.cpu_count())\n",
    "results_object = [craft_pool.apply_async(craft_sample, args=(x, y, grad)) \\\n",
    "                      for x, y, grad in zip(xs, ys, x_gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array([r.get() for r in results_object])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # word = 0\n",
    "# for word in range(maxlen):\n",
    "#     # word += 1\n",
    "#     word_grad = x_gradients[0, np.argmax(y), 0, word]\n",
    "#     # print(word_grad.shape)\n",
    "#\n",
    "#     jac_sign = np.sign(word_grad).sum()\n",
    "#     vocab_sign = np.add.reduce(np.sign(vocab_embeddings - word_grad),1)\n",
    "#\n",
    "#     match_word = np.argmin(vocab_sign - jac_sign)\n",
    "#     x[word] = match_word\n",
    "#\n",
    "#     loss , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "#     # print(match_word, loss, acc)\n",
    "#     if acc<1.0 : break\n",
    "#\n",
    "# print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vocab_sign = np.add.reduce(np.sign(vocab_embeddings - word_grad),1)\n",
    "# match_word = np.argmin(vocab_sign - jac_sign)\n",
    "# print(match_word)\n",
    "# x[word] = match_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# _, acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,2))\n",
    "# imdb_clf.predict(x.reshape(-1,maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# idx = np.random.choice(x_train.shape[0], 10)\n",
    "# x, y = x_train[idx], y_train[idx]\n",
    "#\n",
    "# x_embed = get_embeddings(x)\n",
    "# x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "# x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "#\n",
    "# with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.watch(x_var)\n",
    "#     # Get logits\n",
    "#     pred_y = embed_model(x_var)\n",
    "#\n",
    "# # Calculate gradients\n",
    "# y_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "# print(y_gradients.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
