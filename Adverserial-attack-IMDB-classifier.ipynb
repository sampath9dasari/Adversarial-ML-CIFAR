{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Reshape, Activation, Input\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences, with shape (25000,)\n",
      "25000 test sequences with shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=1)\n",
    "print(len(x_train), 'train sequences, with shape', x_train.shape)\n",
    "print(len(x_test), 'test sequences with shape', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decreasing the size of test set.\n",
    "np.random.seed(10)\n",
    "idx = np.random.choice(x_test.shape[0],2000)\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data review statistics:\n",
      "count    25000.000000\n",
      "mean       238.713640\n",
      "std        176.497204\n",
      "min         11.000000\n",
      "25%        130.000000\n",
      "50%        178.000000\n",
      "75%        291.000000\n",
      "max       2494.000000\n",
      "dtype: float64\n",
      "\n",
      "Test data review statistics:\n",
      "count    2000.000000\n",
      "mean      229.910000\n",
      "std       167.108499\n",
      "min        29.000000\n",
      "25%       127.000000\n",
      "50%       174.000000\n",
      "75%       284.000000\n",
      "max      1158.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000, 2)\n",
      "test labels shape: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 240)\n",
      "test data shape: (2000, 240)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 240\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 512\n",
    "embedding_size = 128\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 8\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordnum = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(wordnum)\n",
    "lstm_out = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 240)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 240, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,691,842\n",
      "Trainable params: 2,691,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_clf = Model(inputs=wordnum, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 168s 7ms/sample - loss: 0.0773 - accuracy: 0.9781 - val_loss: 0.7068 - val_accuracy: 0.8375\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 164s 7ms/sample - loss: 0.0650 - accuracy: 0.9829 - val_loss: 0.7074 - val_accuracy: 0.8355\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 170s 7ms/sample - loss: 0.0564 - accuracy: 0.9856 - val_loss: 0.7449 - val_accuracy: 0.8345\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 168s 7ms/sample - loss: 0.0497 - accuracy: 0.9876 - val_loss: 0.7745 - val_accuracy: 0.8305\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 167s 7ms/sample - loss: 0.0445 - accuracy: 0.9899 - val_loss: 0.7885 - val_accuracy: 0.8280\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 171s 7ms/sample - loss: 0.0392 - accuracy: 0.9914 - val_loss: 0.8169 - val_accuracy: 0.8300\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 164s 7ms/sample - loss: 0.0357 - accuracy: 0.9923 - val_loss: 0.8263 - val_accuracy: 0.8315\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 160s 6ms/sample - loss: 0.0342 - accuracy: 0.9928 - val_loss: 0.8298 - val_accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train,\n",
    "                             validation_data=(x_test, y_test),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate over Test data:\n",
      "2000/2000 [==============================] - 17s 8ms/sample - loss: 0.8298 - accuracy: 0.8300\n",
      "Loss over Test data: 0.8298312134742737\n",
      "Accuracy over Test data: 0.83\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate over Test data:\")\n",
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test)\n",
    "print('Loss over Test data:', loss)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (20000, 128)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (240, 128)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf.save(\"imdb_compiled_clf.h5\")\n",
    "imdb_clf.save_weights(\"imdb_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sub-model - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 240, 128)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 131,842\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    # if not compare_losses(x, y, pred_y) : return None\n",
    "    return x_gradients\n",
    "\n",
    "def compare_losses(x, labels, preds):\n",
    "    # Calculate loss\n",
    "    calc_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=preds)\n",
    "    model_loss, _ = imdb_clf.evaluate(x.reshape(-1,maxlen),labels.reshape(-1,num_classes))\n",
    "\n",
    "    return calc_loss-model_loss<0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "\n",
    "    _ , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "\n",
    "    for word in range(maxlen):\n",
    "\n",
    "        if acc<1.0 : break\n",
    "\n",
    "        word_grad = x_gradient[np.argmax(y), word]\n",
    "        # print(word_grad.shape)\n",
    "\n",
    "#         jac_sign = np.sign(word_grad).sum()\n",
    "#         vocab_sign = np.add.reduce(np.sign(word_grad - vocab_embeddings),1)\n",
    "        jac_sign = np.sign(word_grad)\n",
    "        vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "\n",
    "#         match_word = np.argmin(vocab_sign - jac_sign)\n",
    "        match_word = np.argmin(np.add.reduce(vocab_sign - jac_sign, axis=1))\n",
    "        x[word] = match_word\n",
    "\n",
    "        loss , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "        \n",
    "#     print(word,acc)\n",
    "    if acc<1.0:\n",
    "        return x, word\n",
    "    else:\n",
    "        return  x_copy, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "idx = np.random.choice(x_train.shape[0], 10)\n",
    "xs, ys = x_train[idx], y_train[idx]\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "# print(imdb_clf.evaluate(xs, ys))\n",
    "\n",
    "print(\"Crafting adversarial samples...\")\n",
    "for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "    # x = x_train[idx]\n",
    "    # y = y_train[idx]\n",
    "    new_x , changes = craft_sample(x, y, grad)\n",
    "    crafted_x.append(new_x)\n",
    "    num_changes.append(changes)\n",
    "\n",
    "crafted_x = np.array(crafted_x)\n",
    "num_changes = np.array(num_changes)\n",
    "\n",
    "print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "imdb_clf.evaluate(crafted_x, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "(10, 2, 80, 128)\n",
      "Crafting adversarial samples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "idx = np.random.choice(x_train.shape[0], 10)\n",
    "xs, ys = x_train[idx], y_train[idx]\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "print(\"Crafting adversarial samples...\")\n",
    "craft_pool = mp.Pool(mp.cpu_count())\n",
    "results_object = [craft_pool.apply_async(craft_sample, args=(x, y, grad)) \\\n",
    "                      for x, y, grad in zip(xs, ys, x_gradients)]\n",
    "\n",
    "results = np.array([r.get() for r in results_object])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array([r.get() for r in results_object])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "Shape of the Jacobian: (1, 2, 80, 128)\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(x_train.shape[0], 1)\n",
    "x, y = x_train[idx], y_train[idx]\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradient = compute_input_jacobian(x,y,embed_model)\n",
    "x_copy = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      " -1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1.  1.\n",
      " -1.  1.] (128,)\n",
      "[[-1. -1. -1. ...  1. -1. -1.]\n",
      " [-1. -1.  1. ...  1.  1. -1.]\n",
      " [-1.  1. -1. ...  1. -1.  1.]\n",
      " ...\n",
      " [ 1. -1.  1. ... -1.  1.  1.]\n",
      " [ 1. -1. -1. ...  1.  1.  1.]\n",
      " [ 1.  1. -1. ...  1. -1.  1.]] (5000, 128)\n"
     ]
    }
   ],
   "source": [
    "x = x_copy.copy()\n",
    "\n",
    "_ , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "\n",
    "word=0\n",
    "# for word in range(maxlen):\n",
    "\n",
    "# if acc<1.0 : break\n",
    "\n",
    "word_grad = x_gradient[0, np.argmax(y), word]\n",
    "# print(word_grad.shape)\n",
    "\n",
    "# jac_sign = np.sign(word_grad).sum()\n",
    "# print(jac_sign)\n",
    "# vocab_sign = np.add.reduce(np.sign(word_grad - vocab_embeddings),1)\n",
    "# print(vocab_sign)\n",
    "jac_sign = np.sign(word_grad)\n",
    "print(jac_sign, jac_sign.shape)\n",
    "vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "print(vocab_sign, vocab_sign.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = vocab_sign - jac_sign\n",
    "np.add.reduce(new,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "match_word = np.argmin(np.add.reduce(vocab_sign - jac_sign, axis=1))\n",
    "#         match_word = np.argmin(np.sum(vocab_sign - jac_sign))\n",
    "x[word] = match_word\n",
    "\n",
    "loss , acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,num_classes), verbose=0)\n",
    "\n",
    "print(word,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vocab_sign = np.add.reduce(np.sign(vocab_embeddings - word_grad),1)\n",
    "# match_word = np.argmin(vocab_sign - jac_sign)\n",
    "# print(match_word)\n",
    "# x[word] = match_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# _, acc = imdb_clf.evaluate(x.reshape(-1,maxlen), y.reshape(-1,2))\n",
    "# imdb_clf.predict(x.reshape(-1,maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# idx = np.random.choice(x_train.shape[0], 10)\n",
    "# x, y = x_train[idx], y_train[idx]\n",
    "#\n",
    "# x_embed = get_embeddings(x)\n",
    "# x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "# x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "#\n",
    "# with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.watch(x_var)\n",
    "#     # Get logits\n",
    "#     pred_y = embed_model(x_var)\n",
    "#\n",
    "# # Calculate gradients\n",
    "# y_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "# print(y_gradients.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
