{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train split:  (25000, 2)\n",
      "Shape of Test split:  (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"As a lifelong fan of Dickens, I have invaria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"I saw this film on True Movies (which automa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>b'This was a wonderfully clever and entertaini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>b'I have no idea what the other reviewer is ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  b\"As a lifelong fan of Dickens, I have invaria...\n",
       "1      1  b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...\n",
       "2      1  b\"I saw this film on True Movies (which automa...\n",
       "3      1  b'This was a wonderfully clever and entertaini...\n",
       "4      1  b'I have no idea what the other reviewer is ta..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_train, tensor_test = tfds.load('imdb_reviews', split=['train','test'])\n",
    "imdb_train = pd.DataFrame(list(tfds.as_numpy(tensor_train)))\n",
    "imdb_test = pd.DataFrame(list(tfds.as_numpy(tensor_test)))\n",
    "print(\"Shape of Train split: \", imdb_train.shape)\n",
    "print(\"Shape of Test split: \", imdb_test.shape)\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Decreasing the size of test set.\n",
    "np.random.seed(10)\n",
    "idx = np.random.choice(imdb_test.shape[0],2000)\n",
    "imdb_test = imdb_test.iloc[idx]\n",
    "imdb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "stopwords_dict = Counter(eng_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in word_tokenize(text.lower()) if word not in stopwords_dict])\n",
    "    return text\n",
    "\n",
    "def html_tags(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "#     punctuations = string.punctuation.replace('\\'','')+'\\t\\n'\n",
    "    punctuations = string.punctuation+'\\t\\n'\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.8 s, sys: 0 ns, total: 31.8 s\n",
      "Wall time: 31.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ptext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"As a lifelong fan of Dickens, I have invaria...</td>\n",
       "      <td>lifelong fan dickens invariably disappointed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...</td>\n",
       "      <td>oh yeah jenna jameson yeah baby movie rocks on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"I saw this film on True Movies (which automa...</td>\n",
       "      <td>saw film true movies automatically made scepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>b'This was a wonderfully clever and entertaini...</td>\n",
       "      <td>wonderfully clever entertaining movie shall ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>b'I have no idea what the other reviewer is ta...</td>\n",
       "      <td>idea reviewer talking wonderful movie created ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  b\"As a lifelong fan of Dickens, I have invaria...   \n",
       "1      1  b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...   \n",
       "2      1  b\"I saw this film on True Movies (which automa...   \n",
       "3      1  b'This was a wonderfully clever and entertaini...   \n",
       "4      1  b'I have no idea what the other reviewer is ta...   \n",
       "\n",
       "                                               ptext  \n",
       "0  lifelong fan dickens invariably disappointed a...  \n",
       "1  oh yeah jenna jameson yeah baby movie rocks on...  \n",
       "2  saw film true movies automatically made scepti...  \n",
       "3  wonderfully clever entertaining movie shall ne...  \n",
       "4  idea reviewer talking wonderful movie created ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "imdb_train['ptext'] = imdb_train.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(html_tags)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_punctuation)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_stopwords)\n",
    "imdb_test['ptext'] = imdb_test.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(html_tags)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_punctuation)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_stopwords)\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 0 ns, total: 18.4 s\n",
      "Wall time: 18.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75088"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "new_text = imdb_train.ptext.apply(word_tokenize).explode()\n",
    "new_text.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.26 s, sys: 0 ns, total: 3.26 s\n",
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Cut texts after this number of words (among top max_features most common words)\n",
    "max_features = 40000 \n",
    "\n",
    "# Define tokenizer \n",
    "tokenizer = Tokenizer(num_words=max_features,\n",
    "                      lower=True)\n",
    "#                       oov_token=\"<unk>\")\n",
    "\n",
    "# Fit the \n",
    "tokenizer.fit_on_texts(imdb_train.ptext)\n",
    "\n",
    "# Use the '0' index for the padding character\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(imdb_train.ptext)\n",
    "y_train = imdb_train.label.values\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(imdb_test.ptext)\n",
    "y_test = imdb_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data review statistics:\n",
      "count    25000.000000\n",
      "mean       119.364520\n",
      "std         90.134456\n",
      "min          4.000000\n",
      "25%         63.000000\n",
      "50%         88.000000\n",
      "75%        146.000000\n",
      "max       1414.000000\n",
      "dtype: float64\n",
      "\n",
      "Test data review statistics:\n",
      "count    2000.000000\n",
      "mean      112.429000\n",
      "std        83.861394\n",
      "min        10.000000\n",
      "25%        60.000000\n",
      "50%        83.000000\n",
      "75%       138.000000\n",
      "max       516.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000,)\n",
      "test labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train_oe = to_categorical(y_train, 2)\n",
    "y_test_oe = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 70)\n",
      "test data shape: (2000, 70)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 70\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, padding='post', truncating='post', maxlen=maxlen)\n",
    "\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lifelong fan dickens invariably disappointed adaptations novels although works presented extremely accurate telling human life every level victorian britain throughout pervasive thread humour could playful sarcastic narrative dictated way literary cartoonist could serious hilarious sentence pricked pride lampooned arrogance celebrated modesty empathised loneliness poverty may cliché people writer comedy often missing interpretations time writing oliver twist dramatised serial form bbc television misery cruelty non humour irony savage lampoonery result dark',\n",
       " 'oh yeah jenna jameson yeah baby movie rocks one 1st movies saw say feel love great move performance outstanding liked scenery wardrobe amazing tell put lot movie girls cloth amazing hope comment helps u buy movie storyline awesome unique sure u going like jenna amazed us wonder movie many awards make wardrobe sexy girls girls scene amazing specially one looks like angel must see hope u share interests <pad> <pad>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(x_train[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 64\n",
    "embedding_size = 256\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 10\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_encode = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(seq_encode)\n",
    "lstm_out = LSTM(lstm_size)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 70)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 70, 256)           10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10,437,378\n",
      "Trainable params: 10,437,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=1e-3, decay=1e-4)\n",
    "imdb_clf = Model(inputs=seq_encode, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 2000 samples\n",
      "Epoch 1/16\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.4189 - accuracy: 0.8051 - val_loss: 0.3848 - val_accuracy: 0.8260\n",
      "Epoch 2/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1882 - accuracy: 0.9335 - val_loss: 0.4506 - val_accuracy: 0.7975\n",
      "Epoch 3/16\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0919 - accuracy: 0.9702 - val_loss: 0.7158 - val_accuracy: 0.8025\n",
      "Epoch 4/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0436 - accuracy: 0.9874 - val_loss: 0.7380 - val_accuracy: 0.7915\n",
      "Epoch 5/16\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0212 - accuracy: 0.9937 - val_loss: 0.8454 - val_accuracy: 0.7805\n",
      "Epoch 6/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0186 - accuracy: 0.9946 - val_loss: 1.0017 - val_accuracy: 0.7885\n",
      "Epoch 7/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0219 - accuracy: 0.9938 - val_loss: 1.1003 - val_accuracy: 0.7790\n",
      "Epoch 8/16\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.0136 - accuracy: 0.9954 - val_loss: 1.2181 - val_accuracy: 0.7835\n",
      "Epoch 9/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0122 - accuracy: 0.9964 - val_loss: 1.1131 - val_accuracy: 0.7940\n",
      "Epoch 10/16\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0049 - accuracy: 0.9984 - val_loss: 1.2771 - val_accuracy: 0.7890\n",
      "Epoch 11/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0056 - accuracy: 0.9985 - val_loss: 1.2312 - val_accuracy: 0.7900\n",
      "Epoch 12/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0061 - accuracy: 0.9985 - val_loss: 1.3105 - val_accuracy: 0.7885\n",
      "Epoch 13/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0023 - accuracy: 0.9996 - val_loss: 1.4041 - val_accuracy: 0.7865\n",
      "Epoch 14/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0090 - accuracy: 0.9976 - val_loss: 1.3731 - val_accuracy: 0.7855\n",
      "Epoch 15/16\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.3583 - val_accuracy: 0.7885\n",
      "Epoch 16/16\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.4879 - val_accuracy: 0.7830\n"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train_oe,\n",
    "                             validation_data=(x_test, y_test_oe),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate over Test data:\n",
      "2000/2000 [==============================] - 1s 627us/sample - loss: 1.4879 - accuracy: 0.7830\n",
      "Loss over Test data: 1.4879107685089112\n",
      "Accuracy over Test data: 0.783\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate over Test data:\")\n",
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test_oe)\n",
    "print('Loss over Test data:', loss)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (40000, 256)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (70, 256)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Submodel - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "# embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf.save(\"saved_models/imdb_compiled_clf_130dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    return x_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient, max_changes=maxlen):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "    pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "    if pred != y :\n",
    "        return x, 0\n",
    "\n",
    "    for word in range(max_changes):\n",
    "\n",
    "        word_grad = x_gradient[y, word]\n",
    "\n",
    "        jac_sign = np.sign(word_grad)\n",
    "        vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "\n",
    "        match_word = np.argmin(np.absolute(np.add.reduce(vocab_sign - jac_sign, axis=1)))\n",
    "        x_copy[word] = match_word\n",
    "\n",
    "        pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "        if pred != y : break\n",
    "\n",
    "    return  x_copy, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples_class = 20\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "\n",
    "idx0 = np.random.choice(np.argwhere(y_train == 0).reshape(-1,), num_samples_class, replace=False)\n",
    "idx1 = np.random.choice(np.argwhere(y_train == 1).reshape(-1,), num_samples_class, replace=False)\n",
    "idx = np.concatenate((idx0,idx1))\n",
    "# np.random.shuffle(idx)\n",
    "\n",
    "xs, ys, ys_oe = x_train[idx].copy(), y_train[idx].copy(), y_train_oe[idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "Shape of the Jacobian: (40, 2, 70, 256)\n",
      "Loss and accuracy of selected samples: [0.0005068563448730856, 1.0]\n",
      "CPU times: user 27.6 s, sys: 0 ns, total: 27.6 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "print(\"Loss and accuracy of selected samples:\", imdb_clf.evaluate(xs, ys_oe, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting adversarial samples...\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting adversarial samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting with defined number of changes for each sample:\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting with defined number of changes for each sample:\")\n",
    "max_changes = [10,20,30,40,50,60,70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the number of word changes to  10  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:00<00:37,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 2/40 [00:01<00:36,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 3/40 [00:02<00:35,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 4/40 [00:03<00:34,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 5/40 [00:04<00:33,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 6/40 [00:05<00:32,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 7/40 [00:06<00:27,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 8/40 [00:07<00:27,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▎       | 9/40 [00:08<00:27,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 10/40 [00:09<00:27,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 11/40 [00:10<00:26,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 12/40 [00:11<00:26,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▎      | 13/40 [00:11<00:24,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 14/40 [00:12<00:23,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 15/40 [00:13<00:23,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 16/40 [00:14<00:22,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▎     | 17/40 [00:15<00:21,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 18/40 [00:16<00:20,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 19/40 [00:17<00:19,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 20/40 [00:18<00:18,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▎    | 21/40 [00:19<00:17,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 22/40 [00:20<00:16,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▊    | 23/40 [00:21<00:14,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 24/40 [00:21<00:14,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▎   | 25/40 [00:22<00:13,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 26/40 [00:23<00:12,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 27/40 [00:24<00:10,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 28/40 [00:25<00:10,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▎  | 29/40 [00:26<00:09,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 30/40 [00:27<00:09,  1.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 31/40 [00:28<00:08,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 32/40 [00:29<00:07,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▎ | 33/40 [00:30<00:06,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 34/40 [00:31<00:05,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 35/40 [00:31<00:04,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 36/40 [00:32<00:03,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▎| 37/40 [00:33<00:02,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 38/40 [00:34<00:01,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 39/40 [00:35<00:00,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 40/40 [00:36<00:00,  1.09it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      "40/40 [==============================] - 0s 840us/sample - loss: 0.0810 - accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the number of word changes to  20  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▎         | 1/40 [00:01<01:14,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 2/40 [00:03<01:12,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 3/40 [00:05<01:10,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 4/40 [00:07<01:08,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 5/40 [00:09<01:06,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 6/40 [00:11<01:04,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 7/40 [00:11<00:49,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 8/40 [00:13<00:51,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▎       | 9/40 [00:15<00:52,  1.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 10/40 [00:17<00:52,  1.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 11/40 [00:19<00:52,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 12/40 [00:20<00:43,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▎      | 13/40 [00:22<00:42,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 14/40 [00:24<00:43,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 15/40 [00:25<00:37,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 16/40 [00:27<00:39,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▎     | 17/40 [00:28<00:39,  1.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 18/40 [00:30<00:38,  1.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 19/40 [00:32<00:37,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 20/40 [00:34<00:36,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▎    | 21/40 [00:36<00:34,  1.83s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 22/40 [00:38<00:33,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▊    | 23/40 [00:39<00:29,  1.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-afdb6e1b9348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnew_x\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mchanges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_changes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mcrafted_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnum_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-461bcecf329e>\u001b[0m in \u001b[0;36mcraft_sample\u001b[0;34m(x, y, x_gradient, max_changes)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mjac_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mvocab_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_grad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvocab_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmatch_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sign\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mjac_sign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "acc_arr = []\n",
    "for change in max_changes:\n",
    "    print(\"Limiting the number of word changes to \",change,\" in each sample:\")\n",
    "    crafted_x = []\n",
    "    num_changes = []\n",
    "    \n",
    "    for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "        new_x , changes = craft_sample(x, y, grad, max_changes=change)\n",
    "        crafted_x.append(new_x)\n",
    "        num_changes.append(changes)\n",
    "\n",
    "    crafted_x = np.array(crafted_x)\n",
    "    num_changes = np.array(num_changes)\n",
    "    \n",
    "    print(\"Evaluation for the crafted samples:\")\n",
    "    loss, acc = imdb_clf.evaluate(crafted_x, ys_oe)\n",
    "    loss_arr.append(loss)\n",
    "    acc_arr.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting with variable changes for each sample:\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting with variable changes for each sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [14:33<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 45.19\n",
      "200/200 [==============================] - 0s 655us/sample - loss: 1.7113 - accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7112814259529114, 0.2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_x = []\n",
    "num_changes = []\n",
    "for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "    new_x , changes = craft_sample(x, y, grad, fixed_changes=False)\n",
    "    crafted_x.append(new_x)\n",
    "    num_changes.append(changes)\n",
    "\n",
    "crafted_x = np.array(crafted_x)\n",
    "num_changes = np.array(num_changes)\n",
    "\n",
    "print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "imdb_clf.evaluate(crafted_x, ys_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(id):\n",
    "    print(\"Number of changes on the sample:\",num_changes[id])\n",
    "    print(\"Original label: \",ys[id],\" | Predicted label: \",np.argmax(imdb_clf.predict(crafted_x[id].reshape(-1,maxlen))))\n",
    "    print(tokenizer.sequences_to_texts(xs[id].reshape(-1,maxlen)))\n",
    "    print()\n",
    "    print(tokenizer.sequences_to_texts(crafted_x[id].reshape(-1,maxlen)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changes on the sample: 51\n",
      "Original label:  0  | Predicted label:  1\n",
      "['rented movie last week saw kevin spacey morgan freeman seemed promising justin timberlake came scene really bad actor allowed make movie ever mean one boring uninspired actors ever seen puts absolutely emotion lines whatsoever hell cast role josh pollack think matt damon would better choice kevin spacey another big disappointment character dull seems like bad mix character american beauty john doe se7en might sound cool believe dylan mcdermott acting good']\n",
      "\n",
      "['watch movies movies never never something movies one look would people one people make like well people people one one one one one well would could make make hard well make make hard one first make first well use without things see first well role years bad films best watch lot bad seems like bad mix character american beauty john doe se7en might sound cool believe dylan mcdermott acting good']\n"
     ]
    }
   ],
   "source": [
    "compare(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
