{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train split:  (25000, 2)\n",
      "Shape of Test split:  (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"I saw this film on True Movies (which automatically made me sceptical) but actually - it was g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>b'This was a wonderfully clever and entertaining movie that I shall never tire of watching many,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>b'I have no idea what the other reviewer is talking about- this was a wonderful movie, and creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      1   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  b\"As a lifelong fan of Dickens, I have invariably been disappointed by adaptations of his novels...  \n",
       "1  b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies...  \n",
       "2  b\"I saw this film on True Movies (which automatically made me sceptical) but actually - it was g...  \n",
       "3  b'This was a wonderfully clever and entertaining movie that I shall never tire of watching many,...  \n",
       "4  b'I have no idea what the other reviewer is talking about- this was a wonderful movie, and creat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_train, tensor_test = tfds.load('imdb_reviews', split=['train','test'])\n",
    "imdb_train = pd.DataFrame(list(tfds.as_numpy(tensor_train)))\n",
    "imdb_test = pd.DataFrame(list(tfds.as_numpy(tensor_test)))\n",
    "print(\"Shape of Train split: \", imdb_train.shape)\n",
    "print(\"Shape of Test split: \", imdb_test.shape)\n",
    "# with pd.set_option('display.max_colwidth', 100):\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_text = imdb_train.text.apply(lambda x : x.decode('utf-8')).apply(word_tokenize).explode()\n",
    "new_text.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Decreasing the size of test set.\n",
    "# np.random.seed(10)\n",
    "# idx = np.random.choice(imdb_test.shape[0],2000)\n",
    "# imdb_test = imdb_test.iloc[idx]\n",
    "# imdb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "stopwords_dict = Counter(eng_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in word_tokenize(text.lower()) if word not in stopwords_dict])\n",
    "    return text\n",
    "\n",
    "def html_tags(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "#     punctuations = string.punctuation.replace('\\'','')+'\\t\\n'\n",
    "    punctuations = string.punctuation+'\\t\\n'\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imdb_train['ptext'] = imdb_train.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(html_tags)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_punctuation)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_stopwords)\n",
    "imdb_test['ptext'] = imdb_test.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(html_tags)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_punctuation)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_stopwords)\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_text = imdb_train.ptext.apply(word_tokenize).explode()\n",
    "new_text.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Cut texts after this number of words (among top max_features most common words)\n",
    "max_features = 80000 \n",
    "\n",
    "# Define tokenizer \n",
    "tokenizer = Tokenizer(num_words=max_features,\n",
    "                      lower=True)\n",
    "#                       oov_token=\"<unk>\")\n",
    "\n",
    "# Fit the \n",
    "tokenizer.fit_on_texts(imdb_train.ptext)\n",
    "\n",
    "# Use the '0' index for the padding character\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(imdb_train.ptext)\n",
    "y_train = imdb_train.label.values\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(imdb_test.ptext)\n",
    "y_test = imdb_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train_oe = to_categorical(y_train, 2)\n",
    "y_test_oe = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 120\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, padding='post', truncating='post', maxlen=maxlen)\n",
    "\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(x_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 64\n",
    "embedding_size = 256\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 15\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_encode = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(seq_encode)\n",
    "lstm_out = LSTM(lstm_size)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 120)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 120, 256)          20480000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,677,378\n",
      "Trainable params: 20,677,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=1e-3, decay=1e-3)\n",
    "imdb_clf = Model(inputs=seq_encode, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 150s 6ms/sample - loss: 0.6352 - accuracy: 0.6332 - val_loss: 0.5728 - val_accuracy: 0.7394\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.6118 - accuracy: 0.6807 - val_loss: 0.6790 - val_accuracy: 0.6436\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.5232 - accuracy: 0.7526 - val_loss: 0.5556 - val_accuracy: 0.7181\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.5241 - accuracy: 0.7512 - val_loss: 0.5569 - val_accuracy: 0.7315\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.3464 - accuracy: 0.8632 - val_loss: 0.5653 - val_accuracy: 0.7080\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.2322 - accuracy: 0.9160 - val_loss: 0.4182 - val_accuracy: 0.8290\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.1530 - accuracy: 0.9492 - val_loss: 0.4543 - val_accuracy: 0.8275\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 148s 6ms/sample - loss: 0.1120 - accuracy: 0.9659 - val_loss: 0.4812 - val_accuracy: 0.8293\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.0808 - accuracy: 0.9780 - val_loss: 0.5557 - val_accuracy: 0.8331\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 148s 6ms/sample - loss: 0.0625 - accuracy: 0.9844 - val_loss: 0.6184 - val_accuracy: 0.8284\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0485 - accuracy: 0.9892 - val_loss: 0.6154 - val_accuracy: 0.8296\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0397 - accuracy: 0.9919 - val_loss: 0.6750 - val_accuracy: 0.8298\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0347 - accuracy: 0.9931 - val_loss: 0.6682 - val_accuracy: 0.8252\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0310 - accuracy: 0.9939 - val_loss: 0.7259 - val_accuracy: 0.8282\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.0282 - accuracy: 0.9942 - val_loss: 0.7297 - val_accuracy: 0.8269\n"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train_oe,\n",
    "                             validation_data=(x_test, y_test_oe),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/sample - loss: 0.7297 - accuracy: 0.8269\n",
      "Accuracy over Test data: 0.82692\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test_oe)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/sample - loss: 0.0226 - accuracy: 0.9957\n",
      "Accuracy over Train data: 0.99572\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = imdb_clf.evaluate(x_train, y_train_oe)\n",
    "print('Accuracy over Train data:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 33s 1ms/sample - loss: 0.7297 - accuracy: 0.8269\n",
      "Accuracy over Test data: 0.82692\n",
      "25000/25000 [==============================] - 33s 1ms/sample - loss: 0.0226 - accuracy: 0.9957\n",
      "Accuracy over Train data: 0.99572\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = imdb_clf.evaluate(x_test, y_test_oe)\n",
    "print('Accuracy over Test data:', accuracy)\n",
    "\n",
    "_, accuracy = imdb_clf.evaluate(x_train, y_train_oe)\n",
    "print('Accuracy over Train data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf.save(\"saved_models/imdb_compiled_clf_120dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_clf = load_model(\"saved_models/imdb_compiled_clf_70dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (80000, 256)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (120, 256)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Submodel - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "# embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    return x_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient, max_changes=maxlen):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "    pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "    if pred != y :\n",
    "        return x, 0\n",
    "\n",
    "    for word in range(max_changes):\n",
    "\n",
    "        word_grad = x_gradient[y, word]\n",
    "\n",
    "        jac_sign = np.sign(word_grad)\n",
    "        vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "        \n",
    "        sum_up = np.absolute(np.add.reduce(vocab_sign - jac_sign, axis=1))\n",
    "        sum_up[0] = 1000\n",
    "        match_word = np.argmin(sum_up)\n",
    "        x_copy[word] = match_word\n",
    "\n",
    "        pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "        if pred != y : break\n",
    "\n",
    "    return  x_copy, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples_class = 100\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "\n",
    "idx0 = np.random.choice(np.argwhere(y_train == 0).reshape(-1,), num_samples_class, replace=False)\n",
    "idx1 = np.random.choice(np.argwhere(y_train == 1).reshape(-1,), num_samples_class, replace=False)\n",
    "idx = np.concatenate((idx0,idx1))\n",
    "# np.random.shuffle(idx)\n",
    "\n",
    "xs, ys, ys_oe = x_train[idx].copy(), y_train[idx].copy(), y_train_oe[idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "Shape of the Jacobian: (200, 2, 120, 256)\n",
      "Loss and accuracy of selected samples: 0.0068 and 1.0000\n",
      "CPU times: user 1min 8s, sys: 0 ns, total: 1min 8s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "loss, acc = imdb_clf.evaluate(xs, ys_oe, verbose=0)\n",
    "results = [('Original', 0, loss, acc)]\n",
    "print(\"Loss and accuracy of selected samples: %.4f and %.4f\"%(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting adversarial samples...\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting adversarial samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting with defined number of changes for each sample:\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting with defined number of changes for each sample:\")\n",
    "max_changes = [10,20,30,40,60,80,100,120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the number of word changes to  10  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:36<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.1849 - accuracy: 0.8958"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1993 - accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 8.465\n",
      "Limiting the number of word changes to  20  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:52<00:00,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.4151 - accuracy: 0.7604"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.4668 - accuracy: 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 16.55\n",
      "Limiting the number of word changes to  30  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [18:04<00:00,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.6634 - accuracy: 0.6458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.6951 - accuracy: 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 22.97\n",
      "Limiting the number of word changes to  40  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [18:57<00:00,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.9036 - accuracy: 0.5104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.8730 - accuracy: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 28.045\n",
      "Limiting the number of word changes to  60  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:05<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.2729 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.1324 - accuracy: 0.2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 35.04\n",
      "Limiting the number of word changes to  80  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [27:31<00:00,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.4669 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.2776 - accuracy: 0.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 39.375\n",
      "Limiting the number of word changes to  100  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [28:38<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.5381 - accuracy: 0.0208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.3136 - accuracy: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 41.445\n",
      "Limiting the number of word changes to  120  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [29:34<00:00,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.5526 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.3276 - accuracy: 0.0650\n",
      "Average number of changes per sample: 42.85\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "acc_arr = []\n",
    "perturbed_x = []\n",
    "changes_track = []\n",
    "for change in max_changes:\n",
    "    print(\"Limiting the number of word changes to \",change,\" in each sample:\")\n",
    "    crafted_x = []\n",
    "    num_changes = []\n",
    "    \n",
    "    for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "        new_x , changes = craft_sample(x, y, grad, max_changes=change)\n",
    "        crafted_x.append(new_x)\n",
    "        num_changes.append(changes)\n",
    "    \n",
    "    crafted_x = np.array(crafted_x)\n",
    "    num_changes = np.array(num_changes)\n",
    "    \n",
    "    perturbed_x.append(crafted_x)\n",
    "    changes_track.append(num_changes)\n",
    "    \n",
    "    avg_changes = num_changes.mean()\n",
    "    \n",
    "    print(\"Evaluation for the crafted samples:\")\n",
    "    loss, acc = imdb_clf.evaluate(crafted_x, ys_oe)\n",
    "    print(\"Average number of changes per sample:\", avg_changes)\n",
    "    results.append((str(change), avg_changes, loss, acc))\n",
    "    loss_arr.append(loss)\n",
    "    acc_arr.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Crafting with variable changes for each sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crafted_x = []\n",
    "# num_changes = []\n",
    "# for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "#     new_x , changes = craft_sample(x, y, grad)\n",
    "#     crafted_x.append(new_x)\n",
    "#     num_changes.append(changes)\n",
    "\n",
    "# crafted_x = np.array(crafted_x)\n",
    "# num_changes = np.array(num_changes)\n",
    "\n",
    "# print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "# imdb_clf.evaluate(crafted_x, ys_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_x = np.array(perturbed_x)\n",
    "changes_track = np.array(changes_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('crafted x', 'ab') as fo:     \n",
    "    pickle.dump(perturbed_x, fo)    \n",
    "\n",
    "with open('Changes tracker', 'ab') as fo:     \n",
    "    pickle.dump(changes_track, fo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max changes</th>\n",
       "      <th>Average changes</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>8.465</td>\n",
       "      <td>0.199264</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16.550</td>\n",
       "      <td>0.466769</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>22.970</td>\n",
       "      <td>0.695085</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>28.045</td>\n",
       "      <td>0.872956</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>35.040</td>\n",
       "      <td>1.132386</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80</td>\n",
       "      <td>39.375</td>\n",
       "      <td>1.277620</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>41.445</td>\n",
       "      <td>1.313585</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120</td>\n",
       "      <td>42.850</td>\n",
       "      <td>1.327620</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Max changes  Average changes      Loss  Accuracy\n",
       "0    Original            0.000  0.006780     1.000\n",
       "1          10            8.465  0.199264     0.880\n",
       "2          20           16.550  0.466769     0.705\n",
       "3          30           22.970  0.695085     0.575\n",
       "4          40           28.045  0.872956     0.450\n",
       "5          60           35.040  1.132386     0.290\n",
       "6          80           39.375  1.277620     0.140\n",
       "7         100           41.445  1.313585     0.080\n",
       "8         120           42.850  1.327620     0.065"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results,columns=['Max changes', 'Average changes', 'Loss', 'Accuracy'])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"Text clf attack 120dim results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read back pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf = load_model(\"saved_models/imdb_compiled_clf_120dim.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "max_changes = [10,20,30,40,60,80,100,120]\n",
    "with open('crafted x', 'rb') as fo:\n",
    "    perturbed_x = pickle.load(fo)\n",
    "    \n",
    "with open('Changes tracker', 'rb') as fo:\n",
    "    changes_track = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples_class = 100\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "\n",
    "idx0 = np.random.choice(np.argwhere(y_train == 0).reshape(-1,), num_samples_class, replace=False)\n",
    "idx1 = np.random.choice(np.argwhere(y_train == 1).reshape(-1,), num_samples_class, replace=False)\n",
    "idx = np.concatenate((idx0,idx1))\n",
    "# np.random.shuffle(idx)\n",
    "\n",
    "xs, ys, ys_oe = x_train[idx].copy(), y_train[idx].copy(), y_train_oe[idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 200, 120)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(tid):\n",
    "    print(\"Original label:\",ys[tid])\n",
    "    imdb_clf.evaluate(xs[tid].reshape(-1,maxlen), ys_oe[tid].reshape(-1,2))\n",
    "    print(\"Original tweet:\")\n",
    "    print(tokenizer.sequences_to_texts(xs[tid].reshape(-1,maxlen)))\n",
    "    print()\n",
    "    for i in range(len(max_changes)):\n",
    "        max_change = max_changes[i]\n",
    "        pred = np.argmax(imdb_clf.predict(perturbed_x[i,tid].reshape(-1,maxlen)))\n",
    "        if pred != ys[tid]:\n",
    "            print(\"For max changes:\", max_change)\n",
    "            print(\"Number of changes on the sample:\",changes_track[i,tid]+1)\n",
    "            print(\"Original label: \",ys[tid],\" | Predicted label: \", pred)\n",
    "            print(tokenizer.sequences_to_texts(perturbed_x[i,tid].reshape(-1,maxlen)))\n",
    "            break\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 0\n",
      "1/1 [==============================] - 0s 12ms/sample - loss: 9.9990e-04 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['three stooges rocket travel 1959 first feature length film star stooges pretty bad makes three stooges go around world daze 1963 look like masterpiece stooges janitors rocket place climb rocket goes venus meet stuff including talking unicorn call uni bring back earth uni speaks like average pleasant person oh hello lovely planet hope like hilarious gags many scenes go stooges arrive back space film far story goes one told film makers picture continues another 10 minutes party nothing much happens stooges leave party film almost high point film end stooges sing dapper little song journey larry curly joe hit moe face two pies brutal another writer mentioned fine musical score huh music even noticed two classic tunes take romance goes']\n",
      "\n",
      "For max changes: 20\n",
      "Number of changes on the sample: 18\n",
      "Original label:  0  | Predicted label:  1\n",
      "['like see actually films many quickly lot story really us husband people great got great lot best classic world daze 1963 look like masterpiece stooges janitors rocket place climb rocket goes venus meet stuff including talking unicorn call uni bring back earth uni speaks like average pleasant person oh hello lovely planet hope like hilarious gags many scenes go stooges arrive back space film far story goes one told film makers picture continues another 10 minutes party nothing much happens stooges leave party film almost high point film end stooges sing dapper little song journey larry curly joe hit moe face two pies brutal another writer mentioned fine musical score huh music even noticed two classic tunes take romance goes']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 0\n",
      "1/1 [==============================] - 0s 11ms/sample - loss: 0.0065 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['disappearance couple take family vacation new mexico find deep trouble taking detour main highway visit town seemingly abandoned 1948 unknown reasons town weaver seems harmless first tourist appeal family stranded overnight begin good reason suspect others experienced predicament fatal outcomes henleys watch blair witch project esquire video diary left town last victim ironically demonstrates best performance anyone movie although hamlin dey performances much better supporting casts emotional affect seems flat throughout movie disappearance appeal movie much suspense good direction however plot takes unexpected implausible turns seemingly make sense worse yet really understanding exactly going movie makes bizarre ending less tolerable appeared movie makers focused making stream suspenseful scenes threw away elements good story making plot development gradual explanation themes symbols']\n",
      "\n",
      "For max changes: 80\n",
      "Number of changes on the sample: 61\n",
      "Original label:  0  | Predicted label:  1\n",
      "['like time great could good people people two get great people seen make also story time films story people acting story really films seen get really movie see time never get people see really time make story really make going see also film life another people good film life get really great really many see good get see also people great performances much better supporting casts emotional affect seems flat throughout movie disappearance appeal movie much suspense good direction however plot takes unexpected implausible turns seemingly make sense worse yet really understanding exactly going movie makes bizarre ending less tolerable appeared movie makers focused making stream suspenseful scenes threw away elements good story making plot development gradual explanation themes symbols']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 0\n",
      "Original tweet:\n",
      "['wow plot film place much plot many things happen practically made head spin result none seemed particularly believable movie starts kay francis housewife living small town experience local theater ambitions going broadway big time actor arrives town pursues hopes give career boost husband worried shenanigans actor cad hubby bursts hits actor actor dies result convicted first degree murder manslaughter murder 1 pregnant need funds kay goes new york broadway jobs found forced take job even burlesque unable adequately care young daughter gives another woman raise however eventually find job real broadway play everything looks rosy jealous diva starring play hates inexplicable reason forces thrown play despondent makes way england becomes real star years later returns new york get kid child']\n",
      "\n",
      "For max changes: 40\n",
      "Number of changes on the sample: 35\n",
      "Original label:  0  | Predicted label:  1\n",
      "['also like really life plot like characters film one good know like much show bad around film good make good see like like quite really people quite life plot people years years actually set character arrives town pursues hopes give career boost husband worried shenanigans actor cad hubby bursts hits actor actor dies result convicted first degree murder manslaughter murder 1 pregnant need funds kay goes new york broadway jobs found forced take job even burlesque unable adequately care young daughter gives another woman raise however eventually find job real broadway play everything looks rosy jealous diva starring play hates inexplicable reason forces thrown play despondent makes way england becomes real star years later returns new york get kid child']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 0\n",
      "1/1 [==============================] - 0s 12ms/sample - loss: 0.0071 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['sad lucian pintilie stop making movies get worse every time niki flo 2003 depressing stab camera unfortunate many movies made yearly romania worst get sent abroad e g chicago international film festival movie without plot acting script waste time money score 0 02 10 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n",
      "For max changes: 30\n",
      "Number of changes on the sample: 23\n",
      "Original label:  0  | Predicted label:  1\n",
      "['good like like watch like best people film actually lot films really completely love also know best best part scenes many love time get sent abroad e g chicago international film festival movie without plot acting script waste time money score 0 02 10 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 0\n",
      "1/1 [==============================] - 0s 11ms/sample - loss: 0.0018 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['foolish implausible tale redeemed opening scene hard boiled police detective delivers nearly audible lines confirming greatest fears dead perhaps film would saved director forgone dazzling star power martinez favor sadly anonymous actor filled screen brief moment name hack tor street could salvage dishwater film less likely villain committing murder dropping stones quarry unsuspecting diver moment brief promise immense perhaps treated screen time obscure thespian ever sequel ill advised film <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n",
      "For max changes: 60\n",
      "Number of changes on the sample: 50\n",
      "Original label:  0  | Predicted label:  1\n",
      "['watching first watching movie first first movie 10 good acting even much also film two acting scene come also also watch watch job job see great best best people people people people people people much almost people thought people lot people acting many beautiful thought reviews course really great turned stones quarry unsuspecting diver moment brief promise immense perhaps treated screen time obscure thespian ever sequel ill advised film <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3],\n",
       "       [  8],\n",
       "       [ 14],\n",
       "       [ 20],\n",
       "       [ 33],\n",
       "       [ 46],\n",
       "       [ 52],\n",
       "       [ 57],\n",
       "       [ 64],\n",
       "       [ 72],\n",
       "       [ 76],\n",
       "       [ 81],\n",
       "       [ 92],\n",
       "       [101],\n",
       "       [104],\n",
       "       [117],\n",
       "       [138],\n",
       "       [146],\n",
       "       [152],\n",
       "       [160],\n",
       "       [161],\n",
       "       [169],\n",
       "       [177],\n",
       "       [181],\n",
       "       [189]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.argmax(changes_track,axis=0)==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 1\n",
      "1/1 [==============================] - 0s 11ms/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['silverlake life documentary plain straightforward actually like home movie want dramatic illuminations see something else means tearjerker mean positive ways shows two men love afflicted aids affecting quality every day lives almost difficult say whether quality film undressed look ways respond admirable film actually one admirable sincere documents ever seen two men incredible integrity lives reduced basic parts makes hollow wood productions aids seem hip heartless men made movie one best reasons create something scene tom sings sunshine mark tells goodbye real thing <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n",
      "For max changes: 40\n",
      "Number of changes on the sample: 31\n",
      "Original label:  1  | Predicted label:  0\n",
      "['bad movie one much would tv something mind blood watching making saw black fact television watching think think drama money think made get first made get get first better far something almost difficult say whether quality film undressed look ways respond admirable film actually one admirable sincere documents ever seen two men incredible integrity lives reduced basic parts makes hollow wood productions aids seem hip heartless men made movie one best reasons create something scene tom sings sunshine mark tells goodbye real thing <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 1\n",
      "1/1 [==============================] - 0s 13ms/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['first saw movie freshman high school film stuck years soundtrack cinematography even dialogue somewhat bad acting educational purpose message behind important sin child teenager still high school really bad thing either problem tons girls knew children guess never watched great movie clearly get message behind taking chances sexual relationship girl get pregnant first time myth necessarily lose dreams take backseat future child think first movie clear message behind say <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n",
      "For max changes: 10\n",
      "Number of changes on the sample: 4\n",
      "Original label:  1  | Predicted label:  0\n",
      "['first end better better high school film stuck years soundtrack cinematography even dialogue somewhat bad acting educational purpose message behind important sin child teenager still high school really bad thing either problem tons girls knew children guess never watched great movie clearly get message behind taking chances sexual relationship girl get pregnant first time myth necessarily lose dreams take backseat future child think first movie clear message behind say <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: 1\n",
      "1/1 [==============================] - 0s 13ms/sample - loss: 0.2285 - accuracy: 1.0000\n",
      "Original tweet:\n",
      "['movie lots great actors actresses addressed noble issues full emotion direction done well storyline progresses quickly guess better watch 3 hour movie easy movie watch enjoy <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n",
      "For max changes: 10\n",
      "Number of changes on the sample: 1\n",
      "Original label:  1  | Predicted label:  0\n",
      "['story lots great actors actresses addressed noble issues full emotion direction done well storyline progresses quickly guess better watch 3 hour movie easy movie watch enjoy <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
