{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Reshape, Activation, Input\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences, with shape (25000,)\n",
      "25000 test sequences with shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "max_features = 10000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=1)\n",
    "print(len(x_train), 'train sequences, with shape', x_train.shape)\n",
    "print(len(x_test), 'test sequences with shape', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decreasing the size of test set.\n",
    "np.random.seed(10)\n",
    "idx = np.random.choice(x_test.shape[0],2000)\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000, 2)\n",
      "test labels shape: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(\"One-hot encoding of labels\")\n",
    "# y_train = to_categorical(y_train, 2)\n",
    "# y_test = to_categorical(y_test, 2)\n",
    "# print('train labels shape:',y_train.shape)\n",
    "# print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 150)\n",
      "test data shape: (2000, 150)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 150\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "# K.clear_session()\n",
    "batch_size = 64\n",
    "embedding_size = 128\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 12\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf = load_model('imdb_compiled_clf_150dim.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 19s 775us/sample - loss: 0.0121 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.012113170527815818, 0.99752]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_clf.evaluate(x_train, y_train, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (10000, 128)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (150, 128)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sub-model - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 128)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 131,842\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    # if not compare_losses(x, y, pred_y) : return None\n",
    "    return x_gradients\n",
    "\n",
    "def compare_losses(x, labels, preds):\n",
    "    # Calculate loss\n",
    "    calc_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=preds)\n",
    "    model_loss, _ = imdb_clf.evaluate(x.reshape(-1,maxlen),labels.reshape(-1,num_classes))\n",
    "\n",
    "    return calc_loss-model_loss<0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "\n",
    "    for word in range(maxlen):\n",
    "        \n",
    "        pred = np.argmax(imdb_clf.predict_on_batch(x.reshape(-1,maxlen)))\n",
    "        if pred != y : \n",
    "            return x, word\n",
    "\n",
    "        word_grad = x_gradient[y, word]\n",
    "        # print(word_grad.shape)\n",
    "\n",
    "        jac_sign = np.add.reduce(np.sign(word_grad))\n",
    "        vocab_sign = np.add.reduce(np.sign(word_grad - vocab_embeddings),1)\n",
    "#         jac_sign = np.sign(word_grad)\n",
    "#         vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "\n",
    "        match_word = np.argmin(np.absolute(vocab_sign - jac_sign))\n",
    "#         match_word = np.argmin(np.absolute(np.add.reduce(vocab_sign - jac_sign, axis=1)))\n",
    "        x[word] = match_word\n",
    "\n",
    "#         pred = np.argmax(imdb_clf.predict_on_batch(x.reshape(-1,maxlen)))\n",
    "        \n",
    "#     print(word,acc)\n",
    "\n",
    "    return  x_copy, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "Shape of the Jacobian: (512, 2, 150, 128)\n",
      "Loss and accuracy of selected samples: [0.020581036726071034, 0.9980469]\n",
      "CPU times: user 1min 39s, sys: 9.26 s, total: 1min 48s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "# idx = np.random.choice(x_train.shape[0], 10)\n",
    "idx = range(512)\n",
    "xs, ys = x_train[0:512].copy(), y_train[0:512].copy()\n",
    "# xs, ys = x_train.copy(), y_train.copy()\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "\n",
    "print(\"Loss and accuracy of selected samples:\", imdb_clf.evaluate(xs, ys, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted_x = []\n",
    "num_changes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting adversarial samples...\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting adversarial samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "    # x = x_train[idx]\n",
    "    # y = y_train[idx]\n",
    "    new_x , changes = craft_sample(x, y, grad)\n",
    "    crafted_x.append(new_x)\n",
    "    num_changes.append(changes)\n",
    "\n",
    "crafted_x = np.array(crafted_x)\n",
    "num_changes = np.array(num_changes)\n",
    "\n",
    "print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "imdb_clf.evaluate(crafted_x, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Crafted_x_train_512_150dim.pickle', 'ab') as fo:     \n",
    "    pickle.dump(crafted_x, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('crafted_x_train_512_150dim.csv', crafted_x, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.make_batches import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 s, sys: 54.5 s, total: 1min 46s\n",
      "Wall time: 28.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([   1,   13,  219,   14,   11, 7190,   11,   51,  215,   28,   77,\n",
       "          94,  204,  521,   13, 1227, 4985,   83,    4,    2,    7,    4,\n",
       "         750,   12,   16,   15,  163,   13,  774,  110,   12,  237,   21,\n",
       "          62,  119,    8,  121,   81,   25,   79,    6, 1039, 1814,  660,\n",
       "         233,   44,   12,  112, 1967,   42, 3701,   26,   18,   61,  278,\n",
       "          43,    6,  762,    7,    2,  257, 6056,    9,  345,    2,    2,\n",
       "          42,    2,  642,  257,   31,   50,    9,   24,    6,  815,   31,\n",
       "        1926,    4,  636,  720,   18, 1825,   63, 8276, 3045,  944, 7775,\n",
       "         140,  140, 3976,   19,    2, 3507,    2,   62,   28,  814,   12,\n",
       "          69,   29,    4, 3799, 2210,   48,   25,  216,    8,   14,   22,\n",
       "          19,   35,  911,  330,    5,    6,    2,    2,  483,  490,    2,\n",
       "          12,  208,   83,  129,   55,  118, 1029,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0], dtype=int32), 0)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xs, ys = x_train[10].copy(), y_train[10].copy()\n",
    "ys1 = np.array([0,1])\n",
    "craft_sample(xs, ys1, x_gradients[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 53.9 s, total: 1min 40s\n",
      "Wall time: 23.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([   1,   13,  219,   14,   11, 7190,   11,   51,  215,   28,   77,\n",
       "          94,  204,  521,   13, 1227, 4985,   83,    4,    2,    7,    4,\n",
       "         750,   12,   16,   15,  163,   13,  774,  110,   12,  237,   21,\n",
       "          62,  119,    8,  121,   81,   25,   79,    6, 1039, 1814,  660,\n",
       "         233,   44,   12,  112, 1967,   42, 3701,   26,   18,   61,  278,\n",
       "          43,    6,  762,    7,    2,  257, 6056,    9,  345,    2,    2,\n",
       "          42,    2,  642,  257,   31,   50,    9,   24,    6,  815,   31,\n",
       "        1926,    4,  636,  720,   18, 1825,   63, 8276, 3045,  944, 7775,\n",
       "         140,  140, 3976,   19,    2, 3507,    2,   62,   28,  814,   12,\n",
       "          69,   29,    4, 3799, 2210,   48,   25,  216,    8,   14,   22,\n",
       "          19,   35,  911,  330,    5,    6,    2,    2,  483,  490,    2,\n",
       "          12,  208,   83,  129,   55,  118, 1029,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0], dtype=int32), 0)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xs, ys = x_train[10].copy(), y_train[10].copy()\n",
    "craft_sample2(xs, ys, x_gradients[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   14,    9,   66,    6,   55,   78,   20,  138,   86,    7,\n",
       "         32,    4,   65,    9,   78,   12,    9,   35, 4502,   65, 8253,\n",
       "         32, 2580,    7,  183,  295,   15,   97,   57,  281,   12,   43,\n",
       "        186,    6,  355, 2846, 4129,    4,  156,  566,  297,   11,    6,\n",
       "        821, 1377,   36,  566,   60,  740,   17,   35,  284,  144,  138,\n",
       "        122,   13,  818,   14,   20,    5,   51,  215,   13,   81,   19,\n",
       "         12,  150,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
