{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Crafting Adversarial samples with text for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using IMDB review data set that can be classified as either a positive-negative review.\n",
    "\n",
    "The data is available through Keras for retrieval. We can limit the total number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train split:  (25000, 2)\n",
      "Shape of Test split:  (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"As a lifelong fan of Dickens, I have invaria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"I saw this film on True Movies (which automa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>b'This was a wonderfully clever and entertaini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>b'I have no idea what the other reviewer is ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  b\"As a lifelong fan of Dickens, I have invaria...\n",
       "1      1  b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...\n",
       "2      1  b\"I saw this film on True Movies (which automa...\n",
       "3      1  b'This was a wonderfully clever and entertaini...\n",
       "4      1  b'I have no idea what the other reviewer is ta..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_train, tensor_test = tfds.load('imdb_reviews', split=['train','test'])\n",
    "imdb_train = pd.DataFrame(list(tfds.as_numpy(tensor_train)))\n",
    "imdb_test = pd.DataFrame(list(tfds.as_numpy(tensor_test)))\n",
    "print(\"Shape of Train split: \", imdb_train.shape)\n",
    "print(\"Shape of Test split: \", imdb_test.shape)\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Decreasing the size of test set.\n",
    "# np.random.seed(10)\n",
    "# idx = np.random.choice(imdb_test.shape[0],2000)\n",
    "# imdb_test = imdb_test.iloc[idx]\n",
    "# imdb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "stopwords_dict = Counter(eng_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in word_tokenize(text.lower()) if word not in stopwords_dict])\n",
    "    return text\n",
    "\n",
    "def html_tags(text):\n",
    "    text = re.sub('\\<.*?\\>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "#     punctuations = string.punctuation.replace('\\'','')+'\\t\\n'\n",
    "    punctuations = string.punctuation+'\\t\\n'\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 0 ns, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>ptext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"As a lifelong fan of Dickens, I have invaria...</td>\n",
       "      <td>lifelong fan dickens invariably disappointed a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...</td>\n",
       "      <td>oh yeah jenna jameson yeah baby movie rocks on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>b\"I saw this film on True Movies (which automa...</td>\n",
       "      <td>saw film true movies automatically made scepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>b'This was a wonderfully clever and entertaini...</td>\n",
       "      <td>wonderfully clever entertaining movie shall ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>b'I have no idea what the other reviewer is ta...</td>\n",
       "      <td>idea reviewer talking wonderful movie created ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  b\"As a lifelong fan of Dickens, I have invaria...   \n",
       "1      1  b\"Oh yeah! Jenna Jameson did it again! Yeah Ba...   \n",
       "2      1  b\"I saw this film on True Movies (which automa...   \n",
       "3      1  b'This was a wonderfully clever and entertaini...   \n",
       "4      1  b'I have no idea what the other reviewer is ta...   \n",
       "\n",
       "                                               ptext  \n",
       "0  lifelong fan dickens invariably disappointed a...  \n",
       "1  oh yeah jenna jameson yeah baby movie rocks on...  \n",
       "2  saw film true movies automatically made scepti...  \n",
       "3  wonderfully clever entertaining movie shall ne...  \n",
       "4  idea reviewer talking wonderful movie created ...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "imdb_train['ptext'] = imdb_train.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(html_tags)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_punctuation)\n",
    "imdb_train['ptext'] = imdb_train.ptext.apply(remove_stopwords)\n",
    "imdb_test['ptext'] = imdb_test.text.apply(lambda x : x.decode('utf-8'))\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(html_tags)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_punctuation)\n",
    "imdb_test['ptext'] = imdb_test.ptext.apply(remove_stopwords)\n",
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 s, sys: 0 ns, total: 24.3 s\n",
      "Wall time: 24.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75088"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "new_text = imdb_train.ptext.apply(word_tokenize).explode()\n",
    "new_text.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 s, sys: 0 ns, total: 4.79 s\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Cut texts after this number of words (among top max_features most common words)\n",
    "max_features = 80000 \n",
    "\n",
    "# Define tokenizer \n",
    "tokenizer = Tokenizer(num_words=max_features,\n",
    "                      lower=True)\n",
    "#                       oov_token=\"<unk>\")\n",
    "\n",
    "# Fit the \n",
    "tokenizer.fit_on_texts(imdb_train.ptext)\n",
    "\n",
    "# Use the '0' index for the padding character\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(imdb_train.ptext)\n",
    "y_train = imdb_train.label.values\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(imdb_test.ptext)\n",
    "y_test = imdb_test.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data review statistics:\n",
      "count    25000.000000\n",
      "mean       121.044040\n",
      "std         91.563644\n",
      "min          4.000000\n",
      "25%         64.000000\n",
      "50%         90.000000\n",
      "75%        148.000000\n",
      "max       1435.000000\n",
      "dtype: float64\n",
      "\n",
      "Test data review statistics:\n",
      "count    25000.000000\n",
      "mean       116.113920\n",
      "std         87.130015\n",
      "min          3.000000\n",
      "25%         63.000000\n",
      "50%         87.000000\n",
      "75%        141.000000\n",
      "max       1122.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_train]))\n",
    "print(pdlen.describe())\n",
    "print()\n",
    "print(\"Test data review statistics:\")\n",
    "pdlen = pd.Series(np.array([len(x) for x in x_test]))\n",
    "print(pdlen.describe())\n",
    "# print(\"Average number of words in each review:\", lens.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to one-hot encode the labels, to use probabilities/logits for different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n",
      "train labels shape: (25000,)\n",
      "test labels shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train_oe = to_categorical(y_train, 2)\n",
    "y_test_oe = to_categorical(y_test, 2)\n",
    "print('train labels shape:',y_train.shape)\n",
    "print('test labels shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Keras Embedding layer expects the input to have similar length for each review.\n",
    "So we either need to pad or truncate the reviews as necessary.\n",
    "\n",
    "We are padding/truncating at the end of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (25000, 120)\n",
      "test data shape: (25000, 120)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 120\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', truncating='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, padding='post', truncating='post', maxlen=maxlen)\n",
    "\n",
    "print('train data shape:', x_train.shape)\n",
    "print('test data shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lifelong fan dickens invariably disappointed adaptations novels although works presented extremely accurate telling human life every level victorian britain throughout pervasive thread humour could playful sarcastic narrative dictated way literary caricaturist cartoonist could serious hilarious sentence pricked pride lampooned arrogance celebrated modesty empathised loneliness poverty may cliché people writer comedy often missing interpretations time writing oliver twist dramatised serial form bbc television misery cruelty non humour irony savage lampoonery result dark dismal experience story penned journalist rather novelist really dickens oliver hand much closer mark mockery officialdom perfectly interpreted blustering beadle drunken magistrate classic stand beadle mr brownlow law described ass idiot better done harry secombe ideal choice blinding cruelty also callous indifference state cold hunger poverty loneliness presented surely',\n",
       " 'oh yeah jenna jameson yeah baby movie rocks one 1st movies saw say feel love great move performance outstanding liked scenery wardrobe amazing tell put lot movie girls cloth amazing hope comment helps u buy movie storyline awesome unique sure u going like jenna amazed us wonder movie many awards make wardrobe sexy girls girls scene amazing specially one looks like angel must see hope u share interests <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(x_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model-specific variables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model-specific variables...\")\n",
    "K.clear_session()\n",
    "batch_size = 64\n",
    "embedding_size = 256\n",
    "lstm_size = 128\n",
    "val_split = 0.2\n",
    "epochs = 15\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_encode = Input(shape=(maxlen,))\n",
    "embeddings = Embedding(max_features, embedding_size)(seq_encode)\n",
    "lstm_out = LSTM(lstm_size)(embeddings)\n",
    "dense_out = Dense(num_classes)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 120)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 120, 256)          20480000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 20,677,378\n",
      "Trainable params: 20,677,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=1e-3, decay=1e-3)\n",
    "imdb_clf = Model(inputs=seq_encode, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 150s 6ms/sample - loss: 0.6352 - accuracy: 0.6332 - val_loss: 0.5728 - val_accuracy: 0.7394\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.6118 - accuracy: 0.6807 - val_loss: 0.6790 - val_accuracy: 0.6436\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.5232 - accuracy: 0.7526 - val_loss: 0.5556 - val_accuracy: 0.7181\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.5241 - accuracy: 0.7512 - val_loss: 0.5569 - val_accuracy: 0.7315\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.3464 - accuracy: 0.8632 - val_loss: 0.5653 - val_accuracy: 0.7080\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.2322 - accuracy: 0.9160 - val_loss: 0.4182 - val_accuracy: 0.8290\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.1530 - accuracy: 0.9492 - val_loss: 0.4543 - val_accuracy: 0.8275\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 148s 6ms/sample - loss: 0.1120 - accuracy: 0.9659 - val_loss: 0.4812 - val_accuracy: 0.8293\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.0808 - accuracy: 0.9780 - val_loss: 0.5557 - val_accuracy: 0.8331\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 148s 6ms/sample - loss: 0.0625 - accuracy: 0.9844 - val_loss: 0.6184 - val_accuracy: 0.8284\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0485 - accuracy: 0.9892 - val_loss: 0.6154 - val_accuracy: 0.8296\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0397 - accuracy: 0.9919 - val_loss: 0.6750 - val_accuracy: 0.8298\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0347 - accuracy: 0.9931 - val_loss: 0.6682 - val_accuracy: 0.8252\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 146s 6ms/sample - loss: 0.0310 - accuracy: 0.9939 - val_loss: 0.7259 - val_accuracy: 0.8282\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 147s 6ms/sample - loss: 0.0282 - accuracy: 0.9942 - val_loss: 0.7297 - val_accuracy: 0.8269\n"
     ]
    }
   ],
   "source": [
    "train_history = imdb_clf.fit(x_train, y_train_oe,\n",
    "                             validation_data=(x_test, y_test_oe),\n",
    "#                              validation_split=val_split,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/sample - loss: 0.7297 - accuracy: 0.8269\n",
      "Accuracy over Test data: 0.82692\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = imdb_clf.evaluate(x_test, y_test_oe)\n",
    "print('Accuracy over Test data:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/sample - loss: 0.0226 - accuracy: 0.9957\n",
      "Accuracy over Train data: 0.99572\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = imdb_clf.evaluate(x_train, y_train_oe)\n",
    "print('Accuracy over Train data:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over Test data: 0.82692\n",
      "Accuracy over Train data: 0.99572\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = imdb_clf.evaluate(x_test, y_test_oe, verbose=0)\n",
    "print('Accuracy over Test data:', accuracy)\n",
    "\n",
    "_, accuracy = imdb_clf.evaluate(x_train, y_train_oe, verbose=0)\n",
    "print('Accuracy over Train data:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clf.save(\"saved_models/imdb_compiled_clf_120dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_clf = load_model(\"saved_models/imdb_compiled_clf_70dim.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve Embeddings for all the words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the generated embeddings:  (80000, 256)\n"
     ]
    }
   ],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()\n",
    "print(\"Shape of the generated embeddings: \",vocab_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Keras function to extract embeddings for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the embedding function with a single sample...\n",
      "Shape of generated embeddings: (120, 256)\n"
     ]
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "\n",
    "print(\"Testing the embedding function with a single sample...\")\n",
    "test_embed = get_embeddings(x_test[0])\n",
    "print(\"Shape of generated embeddings:\",test_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining Submodel - from Embeddings to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Defining necessary layers\n",
    "embed_input = Input(shape=test_embed.shape)\n",
    "embed_lstm = LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(num_classes)(embed_lstm)\n",
    "\n",
    "### Define model with Embedding inputs and Logit outputs\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "\n",
    "### Transferring the trained weights from our IMDB Classifier model (imdb_clf)\n",
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())\n",
    "# embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adversarial crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Jacobian matrix for all the words in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_input_jacobian(x, y, model):\n",
    "    x_embed = get_embeddings(x)\n",
    "    x_tensor = tf.convert_to_tensor(x_embed.reshape(-1,maxlen,embedding_size), tf.float32)\n",
    "    x_var = tf.Variable(x_tensor, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(x_var)\n",
    "        # Get logits\n",
    "        pred_y = model(x_var)\n",
    "\n",
    "    # Calculate gradients\n",
    "    x_gradients = tape.batch_jacobian(pred_y, x_var).numpy()\n",
    "    print(\"Shape of the Jacobian:\", x_gradients.shape)\n",
    "\n",
    "    return x_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def craft_sample(x, y, x_gradient, max_changes=maxlen):\n",
    "\n",
    "    x_copy = x.copy()\n",
    "    pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "    if pred != y :\n",
    "        return x, 0\n",
    "\n",
    "    for word in range(max_changes):\n",
    "\n",
    "        word_grad = x_gradient[y, word]\n",
    "\n",
    "        jac_sign = np.sign(word_grad)\n",
    "        vocab_sign = np.sign(word_grad - vocab_embeddings)\n",
    "        \n",
    "        sum_up = np.absolute(np.add.reduce(vocab_sign - jac_sign, axis=1))\n",
    "        sum_up[0] = 1000\n",
    "        match_word = np.argmin(sum_up)\n",
    "        x_copy[word] = match_word\n",
    "\n",
    "        pred = np.argmax(imdb_clf.predict_on_batch(x_copy.reshape(-1,maxlen)))\n",
    "        if pred != y : break\n",
    "\n",
    "    return  x_copy, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples_class = 100\n",
    "\n",
    "crafted_x = []\n",
    "num_changes = []\n",
    "\n",
    "idx0 = np.random.choice(np.argwhere(y_train == 0).reshape(-1,), num_samples_class, replace=False)\n",
    "idx1 = np.random.choice(np.argwhere(y_train == 1).reshape(-1,), num_samples_class, replace=False)\n",
    "idx = np.concatenate((idx0,idx1))\n",
    "# np.random.shuffle(idx)\n",
    "\n",
    "xs, ys, ys_oe = x_train[idx].copy(), y_train[idx].copy(), y_train_oe[idx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating gradients...\n",
      "Shape of the Jacobian: (200, 2, 120, 256)\n",
      "Loss and accuracy of selected samples: 0.0068 and 1.0000\n",
      "CPU times: user 1min 8s, sys: 0 ns, total: 1min 8s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Calculating gradients...\")\n",
    "x_gradients = compute_input_jacobian(xs,ys,embed_model)\n",
    "loss, acc = imdb_clf.evaluate(xs, ys_oe, verbose=0)\n",
    "results = [('Original', 0, loss, acc)]\n",
    "print(\"Loss and accuracy of selected samples: %.4f and %.4f\"%(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting adversarial samples...\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting adversarial samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafting with defined number of changes for each sample:\n"
     ]
    }
   ],
   "source": [
    "print(\"Crafting with defined number of changes for each sample:\")\n",
    "max_changes = [10,20,30,40,60,80,100,120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the number of word changes to  10  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [07:36<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.1849 - accuracy: 0.8958"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1993 - accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 8.465\n",
      "Limiting the number of word changes to  20  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:52<00:00,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.4151 - accuracy: 0.7604"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.4668 - accuracy: 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 16.55\n",
      "Limiting the number of word changes to  30  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [18:04<00:00,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.6634 - accuracy: 0.6458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.6951 - accuracy: 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 22.97\n",
      "Limiting the number of word changes to  40  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [18:57<00:00,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 0.9036 - accuracy: 0.5104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.8730 - accuracy: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 28.045\n",
      "Limiting the number of word changes to  60  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [24:05<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.2729 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.1324 - accuracy: 0.2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 35.04\n",
      "Limiting the number of word changes to  80  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [27:31<00:00,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.4669 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.2776 - accuracy: 0.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 39.375\n",
      "Limiting the number of word changes to  100  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [28:38<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.5381 - accuracy: 0.0208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.3136 - accuracy: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of changes per sample: 41.445\n",
      "Limiting the number of word changes to  120  in each sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [29:34<00:00,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the crafted samples:\n",
      " 96/200 [=============>................] - ETA: 0s - loss: 1.5526 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/sample - loss: 1.3276 - accuracy: 0.0650\n",
      "Average number of changes per sample: 42.85\n"
     ]
    }
   ],
   "source": [
    "loss_arr = []\n",
    "acc_arr = []\n",
    "perturbed_x = []\n",
    "changes_track = []\n",
    "for change in max_changes:\n",
    "    print(\"Limiting the number of word changes to \",change,\" in each sample:\")\n",
    "    crafted_x = []\n",
    "    num_changes = []\n",
    "    \n",
    "    for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "        new_x , changes = craft_sample(x, y, grad, max_changes=change)\n",
    "        crafted_x.append(new_x)\n",
    "        num_changes.append(changes)\n",
    "    \n",
    "    crafted_x = np.array(crafted_x)\n",
    "    num_changes = np.array(num_changes)\n",
    "    \n",
    "    perturbed_x.append(crafted_x)\n",
    "    changes_track.append(num_changes)\n",
    "    \n",
    "    avg_changes = num_changes.mean()\n",
    "    \n",
    "    print(\"Evaluation for the crafted samples:\")\n",
    "    loss, acc = imdb_clf.evaluate(crafted_x, ys_oe)\n",
    "    print(\"Average number of changes per sample:\", avg_changes)\n",
    "    results.append((str(change), avg_changes, loss, acc))\n",
    "    loss_arr.append(loss)\n",
    "    acc_arr.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Crafting with variable changes for each sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crafted_x = []\n",
    "# num_changes = []\n",
    "# for x, y, grad in tqdm(zip(xs, ys, x_gradients), total=xs.shape[0]):\n",
    "#     new_x , changes = craft_sample(x, y, grad)\n",
    "#     crafted_x.append(new_x)\n",
    "#     num_changes.append(changes)\n",
    "\n",
    "# crafted_x = np.array(crafted_x)\n",
    "# num_changes = np.array(num_changes)\n",
    "\n",
    "# print(\"Average number of changes per sample:\", num_changes.mean())\n",
    "\n",
    "# imdb_clf.evaluate(crafted_x, ys_oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_x = np.array(perturbed_x)\n",
    "changes_track = np.array(changes_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('crafted x', 'ab') as fo:     \n",
    "    pickle.dump(perturbed_x, fo)    \n",
    "\n",
    "with open('Changes tracker', 'ab') as fo:     \n",
    "    pickle.dump(changes_track, fo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 200, 120)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max changes</th>\n",
       "      <th>Average changes</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>8.465</td>\n",
       "      <td>0.199264</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>16.550</td>\n",
       "      <td>0.466769</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>22.970</td>\n",
       "      <td>0.695085</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>28.045</td>\n",
       "      <td>0.872956</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>35.040</td>\n",
       "      <td>1.132386</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80</td>\n",
       "      <td>39.375</td>\n",
       "      <td>1.277620</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>41.445</td>\n",
       "      <td>1.313585</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120</td>\n",
       "      <td>42.850</td>\n",
       "      <td>1.327620</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Max changes  Average changes      Loss  Accuracy\n",
       "0    Original            0.000  0.006780     1.000\n",
       "1          10            8.465  0.199264     0.880\n",
       "2          20           16.550  0.466769     0.705\n",
       "3          30           22.970  0.695085     0.575\n",
       "4          40           28.045  0.872956     0.450\n",
       "5          60           35.040  1.132386     0.290\n",
       "6          80           39.375  1.277620     0.140\n",
       "7         100           41.445  1.313585     0.080\n",
       "8         120           42.850  1.327620     0.065"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results,columns=['Max changes', 'Average changes', 'Loss', 'Accuracy'])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"Text clf attack 120dim results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(tid, change_index=0):\n",
    "    max_change = max_changes[change_index]\n",
    "    print(\"Number of changes on the sample:\",changes_track[change_index,tid])\n",
    "    print(\"Original label: \",ys[tid],\" | Predicted label: \",np.argmax(imdb_clf.predict(perturbed_x[change_index,tid].reshape(-1,maxlen))))\n",
    "    print(tokenizer.sequences_to_texts(xs[tid].reshape(-1,maxlen)))\n",
    "    print()\n",
    "    print(tokenizer.sequences_to_texts(perturbed_x[change_index,tid].reshape(-1,maxlen)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changes on the sample: 39\n",
      "Original label:  0  | Predicted label:  0\n",
      "['disappearance couple take family vacation new mexico find deep trouble taking detour main highway visit town seemingly abandoned 1948 unknown reasons town weaver seems harmless first tourist appeal family stranded overnight begin good reason suspect others experienced predicament fatal outcomes henleys watch blair witch project esquire video diary left town last victim ironically demonstrates best performance anyone movie although hamlin dey performances much better supporting casts emotional affect seems flat throughout movie disappearance appeal movie much suspense good direction however plot takes unexpected implausible turns seemingly make sense worse yet really understanding exactly going movie makes bizarre ending less tolerable appeared movie makers focused making stream suspenseful scenes threw away elements good story making plot development gradual explanation themes symbols']\n",
      "\n",
      "['like time great could good people people two get great people seen make also story time films story people acting story really films seen get really movie see time never get people see really time make story really make going henleys watch blair witch project esquire video diary left town last victim ironically demonstrates best performance anyone movie although hamlin dey performances much better supporting casts emotional affect seems flat throughout movie disappearance appeal movie much suspense good direction however plot takes unexpected implausible turns seemingly make sense worse yet really understanding exactly going movie makes bizarre ending less tolerable appeared movie makers focused making stream suspenseful scenes threw away elements good story making plot development gradual explanation themes symbols']\n"
     ]
    }
   ],
   "source": [
    "compare(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
