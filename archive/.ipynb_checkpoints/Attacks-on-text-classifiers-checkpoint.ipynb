{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Reshape, Activation, Input\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import sequence\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Embedding\n",
    "# from keras.layers import LSTM, Reshape, Activation, Input\n",
    "# from keras.datasets import imdb\n",
    "# from keras.utils import to_categorical\n",
    "# import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import theano\n",
    "# import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "CPU times: user 4.22 s, sys: 299 ms, total: 4.52 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_features = 2500\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, seed=1)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in each review: 238.71364\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(x) for x in x_train])\n",
    "\n",
    "print(\"Average number of words in each review:\", lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding of labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"One-hot encoding of labels\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 80\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# need to some setup so everything gets excecuted in the same tensorflow session\n",
    "# myGraph = tf.Graph()\n",
    "# session = tf.Session(graph=myGraph )\n",
    "K.clear_session()\n",
    "# K.set_session( session )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Input(shape=(80,))\n",
    "embeddings = Embedding(max_features, 128)(sentence)\n",
    "lstm_out = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embeddings)\n",
    "dense_out = Dense(2)(lstm_out)\n",
    "out = Activation('softmax')(dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 80)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 80, 128)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 369,538\n",
      "Trainable params: 369,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imdb_clf = Model(inputs=sentence, outputs=out)\n",
    "imdb_clf.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_clf._layers.pop(0)\n",
    "# imdb_clf._layers.pop(0)\n",
    "# imdb_clf.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print('Build model...')\n",
    "# imdb_classifier = Sequential()\n",
    "# imdb_classifier.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "# imdb_classifier.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# # imdb_classifier.add(Reshape(target_shape=(1,128,1)))\n",
    "# # imdb_classifier.add(Dense(2, activation='linear'))\n",
    "# # imdb_classifier.add(TimeDistributed(Dense(2)))\n",
    "# # imdb_classifier.add(Reshape(target_shape=(1,2)))\n",
    "# imdb_classifier.add(Dense(2))\n",
    "# imdb_classifier.add(Activation('softmax'))\n",
    "\n",
    "# # try using different optimizers and different optimizer configs\n",
    "# imdb_classifier.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "          \n",
    "# imdb_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "25000/25000 [==============================] - 25s 992us/sample - loss: 0.5342 - accuracy: 0.7289 - val_loss: 0.3955 - val_accuracy: 0.8224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x648037690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "print('Train...')\n",
    "imdb_clf.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imdb_clf.trainable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = Input(shape=(80,))\n",
    "# embeddings = Embedding(max_features, 128)(sentence)\n",
    "\n",
    "# generate_embeddings = Model(inputs=imdb_clf.inputs, outputs=imdb_clf.layers[1].output)\n",
    "# generate_embeddings.predict(x_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_clf._layers.pop(0)\n",
    "# imdb_clf._layers.pop(0)\n",
    "# imdb_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 80, 128)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 49,538\n",
      "Trainable params: 49,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_input = Input(shape=layer_output[0].shape)\n",
    "embed_lstm = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(embed_input)\n",
    "embed_dense = Dense(2)(embed_lstm)\n",
    "\n",
    "embed_model = Model(inputs=embed_input, outputs=embed_dense)\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 80, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embeddings = K.function([imdb_clf.layers[0].input],\n",
    "                                  imdb_clf.layers[1].output)\n",
    "layer_output = get_embeddings(x_test[0:5])\n",
    "layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_embeddings = imdb_clf.layers[1].embeddings.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grad_model = Model(inputs=imdb_clf.inputs,outputs=imdb_clf.layers[-2].output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # an input layer to feed labels\n",
    "# y_true = Input(shape=(2,))\n",
    "# # compute loss based on model's output and true labels\n",
    "# ce = K.categorical_crossentropy(y_true, imdb_clf.output)\n",
    "# # compute gradient of loss with respect to inputs\n",
    "# grad_ce = K.gradients(ce, imdb_clf.inputs)\n",
    "# # create a function to be able to run this computation graph\n",
    "# func = K.function(imdb_clf.inputs + [y_true], grad_ce)\n",
    "\n",
    "# # usage\n",
    "# output = func([sample, real_y])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample = x_test[0:5]\n",
    "# real_y = y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output_layer = imdb_clf.outputs\n",
    "# all_layers = [layer.output for layer in imdb_clf.layers]\n",
    "# new_model = Model(inputs=imdb_clf.inputs, outputs=all_layers)\n",
    "\n",
    "# sample = tf.convert_to_tensor(x_test[0:5], tf.float32)\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "# # inputs = tf.ones((1, 299, 299, 3))\n",
    "# with tf.GradientTape() as tape:\n",
    "#     output_of_all_layers = new_model(samples)\n",
    "#     preds = output_layer[-1]  # last layer is output layer\n",
    "#     print(preds)\n",
    "#     # take gradients of last layer with respect to all layers in the model\n",
    "#     grads = tape.gradient(preds, output_of_all_layers)\n",
    "#     # note: grads[-1] should be all 1, since it it d(output)/d(output)\n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tf.convert_to_tensor(layer_output[0:5].reshape(-1,80,128), tf.float32)\n",
    "real_y = y_test[0:5]\n",
    "\n",
    "samples = tf.Variable(sample, dtype=tf.float32)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.reset()\n",
    "    tape.watch(samples)\n",
    "#     samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "    print(\"step1\")\n",
    "    # Make prediction\n",
    "    pred_y = embed_model(samples)\n",
    "    print(\"step2\")\n",
    "    # Calculate loss\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    print(\"step3\", model_loss)\n",
    "    print()\n",
    "    #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "\n",
    "# print(tape.watched_variables())\n",
    "# Calculate gradients\n",
    "model_gradients = tape.batch_jacobian(pred_y, samples).numpy()\n",
    "print(model_gradients.reshape(-1,2,80,128).shape)\n",
    "# Update model\n",
    "# optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model_gradients.reshape(-1,2,80,128).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = imdb_classifier.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_logits = K.function([imdb_clf.layers[0].input],[imdb_clf.layers[-1].input])\n",
    "logits = get_logits(x_test[0:5])[0]\n",
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grad_func = K.function([imdb_classifier.layers[1].input], [imdb_classifier.layers[-2].output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_test[0:5], logits=logits)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# samples = x_test[0:5]\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(samples, dtype=tf.float32)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(samples)\n",
    "#     print(\"step1\")\n",
    "#     # Make prediction\n",
    "#     pred_y = grad_model(samples)\n",
    "#     print(\"step2\")\n",
    "#     # Calculate loss\n",
    "#     model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     print(\"step3\")\n",
    "#     print(model_loss)\n",
    "#     #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    \n",
    "# # Calculate gradients\n",
    "# model_gradients = tape.gradient(model_loss, samples)\n",
    "# print(model_gradients)\n",
    "# # Update model\n",
    "# # optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # tf.compat.v1.enable_eager_execution()\n",
    "# samples = tf.convert_to_tensor(layer_output, np.float32)\n",
    "# # inp = tf.Variable(layer_output, dtype=tf.float32)\n",
    "# # labels = tf.convert_to_tensor(y_test[0:10], np.float32)\n",
    "# labels = y_test[0:10]\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(samples)\n",
    "# # def fun(sample):\n",
    "#     logits = grad_model(samples)[0]\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# # grads = tape.gradient(loss, inp)\n",
    "# # tf.test.compute_gradient(fun,[samples])\n",
    "# grads = K.gradients(loss,samples)\n",
    "    \n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from cleverhans.compat import softmax_cross_entropy_with_logits\n",
    "# from cleverhans.utils_keras import KerasModelWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wrapper = KerasModelWrapper( model )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample_ind = 100 # chosen by totaly random dice roll\n",
    "\n",
    "# # picking a test sample\n",
    "# sample = x_train[ sample_ind, : ]\n",
    "# true_label = y_train[ sample_ind, : ]\n",
    "\n",
    "# sample = x_train[ 0:10 ]\n",
    "# true_label = y_train[ 0:10 ]\n",
    "\n",
    "# # sample = sample.reshape(-1,80)\n",
    "# # true_label = true_label.reshape(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# myGraph = tf.Graph()\n",
    "# with myGraph.as_default():\n",
    "#     x = model.layers[ 0 ].input\n",
    "#     y = tf.placeholder( tf.float32, shape=(None, 2) )\n",
    "#     logits = model.layers[-1].input\n",
    "\n",
    "#     loss = softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "#     grads = tf.gradients(loss,x)\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# session.run(init)\n",
    "# output = session.run(grads, feed_dict={x: sample, y : true_label})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grad_model = Model(inputs=imdb_clf.inputs,outputs=imdb_clf.layers[-2].output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # an input layer to feed labels\n",
    "# y_true = Input(shape=(2,))\n",
    "# # compute loss based on model's output and true labels\n",
    "# ce = K.categorical_crossentropy(y_true, imdb_clf.output)\n",
    "# # compute gradient of loss with respect to inputs\n",
    "# grad_ce = K.gradients(ce, imdb_clf.inputs)\n",
    "# # create a function to be able to run this computation graph\n",
    "# func = K.function(imdb_clf.inputs + [y_true], grad_ce)\n",
    "\n",
    "# # usage\n",
    "# output = func([sample, real_y])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample = x_test[0:5]\n",
    "# real_y = y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output_layer = imdb_clf.outputs\n",
    "# all_layers = [layer.output for layer in imdb_clf.layers]\n",
    "# new_model = Model(inputs=imdb_clf.inputs, outputs=all_layers)\n",
    "\n",
    "# sample = tf.convert_to_tensor(x_test[0:5], tf.float32)\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "# # inputs = tf.ones((1, 299, 299, 3))\n",
    "# with tf.GradientTape() as tape:\n",
    "#     output_of_all_layers = new_model(samples)\n",
    "#     preds = output_layer[-1]  # last layer is output layer\n",
    "#     print(preds)\n",
    "#     # take gradients of last layer with respect to all layers in the model\n",
    "#     grads = tape.gradient(preds, output_of_all_layers)\n",
    "#     # note: grads[-1] should be all 1, since it it d(output)/d(output)\n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tf.convert_to_tensor(layer_output[0:5].reshape(-1,80,128), tf.float32)\n",
    "real_y = y_test[0:5]\n",
    "\n",
    "samples = tf.Variable(sample, dtype=tf.float32)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.reset()\n",
    "    tape.watch(samples)\n",
    "#     samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "    print(\"step1\")\n",
    "    # Make prediction\n",
    "    pred_y = embed_model(samples)\n",
    "    print(\"step2\")\n",
    "    # Calculate loss\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    print(\"step3\", model_loss)\n",
    "    print()\n",
    "    #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "\n",
    "# print(tape.watched_variables())\n",
    "# Calculate gradients\n",
    "model_gradients = tape.batch_jacobian(pred_y, samples).numpy()\n",
    "print(model_gradients.reshape(-1,2,80,128).shape)\n",
    "# Update model\n",
    "# optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model_gradients.reshape(-1,2,80,128).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = imdb_classifier.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_logits = K.function([imdb_clf.layers[0].input],[imdb_clf.layers[-1].input])\n",
    "logits = get_logits(x_test[0:5])[0]\n",
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grad_func = K.function([imdb_classifier.layers[1].input], [imdb_classifier.layers[-2].output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_test[0:5], logits=logits)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# samples = x_test[0:5]\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(samples, dtype=tf.float32)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(samples)\n",
    "#     print(\"step1\")\n",
    "#     # Make prediction\n",
    "#     pred_y = grad_model(samples)\n",
    "#     print(\"step2\")\n",
    "#     # Calculate loss\n",
    "#     model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     print(\"step3\")\n",
    "#     print(model_loss)\n",
    "#     #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    \n",
    "# # Calculate gradients\n",
    "# model_gradients = tape.gradient(model_loss, samples)\n",
    "# print(model_gradients)\n",
    "# # Update model\n",
    "# # optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # tf.compat.v1.enable_eager_execution()\n",
    "# samples = tf.convert_to_tensor(layer_output, np.float32)\n",
    "# # inp = tf.Variable(layer_output, dtype=tf.float32)\n",
    "# # labels = tf.convert_to_tensor(y_test[0:10], np.float32)\n",
    "# labels = y_test[0:10]\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(samples)\n",
    "# # def fun(sample):\n",
    "#     logits = grad_model(samples)[0]\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# # grads = tape.gradient(loss, inp)\n",
    "# # tf.test.compute_gradient(fun,[samples])\n",
    "# grads = K.gradients(loss,samples)\n",
    "    \n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from cleverhans.compat import softmax_cross_entropy_with_logits\n",
    "# from cleverhans.utils_keras import KerasModelWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wrapper = KerasModelWrapper( model )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample_ind = 100 # chosen by totaly random dice roll\n",
    "\n",
    "# # picking a test sample\n",
    "# sample = x_train[ sample_ind, : ]\n",
    "# true_label = y_train[ sample_ind, : ]\n",
    "\n",
    "# sample = x_train[ 0:10 ]\n",
    "# true_label = y_train[ 0:10 ]\n",
    "\n",
    "# # sample = sample.reshape(-1,80)\n",
    "# # true_label = true_label.reshape(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# myGraph = tf.Graph()\n",
    "# with myGraph.as_default():\n",
    "#     x = model.layers[ 0 ].input\n",
    "#     y = tf.placeholder( tf.float32, shape=(None, 2) )\n",
    "#     logits = model.layers[-1].input\n",
    "\n",
    "#     loss = softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "#     grads = tf.gradients(loss,x)\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# session.run(init)\n",
    "# output = session.run(grads, feed_dict={x: sample, y : true_label})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grad_model = Model(inputs=imdb_clf.inputs,outputs=imdb_clf.layers[-2].output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # an input layer to feed labels\n",
    "# y_true = Input(shape=(2,))\n",
    "# # compute loss based on model's output and true labels\n",
    "# ce = K.categorical_crossentropy(y_true, imdb_clf.output)\n",
    "# # compute gradient of loss with respect to inputs\n",
    "# grad_ce = K.gradients(ce, imdb_clf.inputs)\n",
    "# # create a function to be able to run this computation graph\n",
    "# func = K.function(imdb_clf.inputs + [y_true], grad_ce)\n",
    "\n",
    "# # usage\n",
    "# output = func([sample, real_y])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample = x_test[0:5]\n",
    "# real_y = y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output_layer = imdb_clf.outputs\n",
    "# all_layers = [layer.output for layer in imdb_clf.layers]\n",
    "# new_model = Model(inputs=imdb_clf.inputs, outputs=all_layers)\n",
    "\n",
    "# sample = tf.convert_to_tensor(x_test[0:5], tf.float32)\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "# # inputs = tf.ones((1, 299, 299, 3))\n",
    "# with tf.GradientTape() as tape:\n",
    "#     output_of_all_layers = new_model(samples)\n",
    "#     preds = output_layer[-1]  # last layer is output layer\n",
    "#     print(preds)\n",
    "#     # take gradients of last layer with respect to all layers in the model\n",
    "#     grads = tape.gradient(preds, output_of_all_layers)\n",
    "#     # note: grads[-1] should be all 1, since it it d(output)/d(output)\n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tf.convert_to_tensor(layer_output[0:5].reshape(-1,80,128), tf.float32)\n",
    "real_y = y_test[0:5]\n",
    "\n",
    "samples = tf.Variable(sample, dtype=tf.float32)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.reset()\n",
    "    tape.watch(samples)\n",
    "#     samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "    print(\"step1\")\n",
    "    # Make prediction\n",
    "    pred_y = embed_model(samples)\n",
    "    print(\"step2\")\n",
    "    # Calculate loss\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    print(\"step3\", model_loss)\n",
    "    print()\n",
    "    #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "\n",
    "# print(tape.watched_variables())\n",
    "# Calculate gradients\n",
    "model_gradients = tape.batch_jacobian(pred_y, samples).numpy()\n",
    "print(model_gradients.reshape(-1,2,80,128).shape)\n",
    "# Update model\n",
    "# optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model_gradients.reshape(-1,2,80,128).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = imdb_classifier.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_logits = K.function([imdb_clf.layers[0].input],[imdb_clf.layers[-1].input])\n",
    "logits = get_logits(x_test[0:5])[0]\n",
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grad_func = K.function([imdb_classifier.layers[1].input], [imdb_classifier.layers[-2].output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_test[0:5], logits=logits)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# samples = x_test[0:5]\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(samples, dtype=tf.float32)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(samples)\n",
    "#     print(\"step1\")\n",
    "#     # Make prediction\n",
    "#     pred_y = grad_model(samples)\n",
    "#     print(\"step2\")\n",
    "#     # Calculate loss\n",
    "#     model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     print(\"step3\")\n",
    "#     print(model_loss)\n",
    "#     #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    \n",
    "# # Calculate gradients\n",
    "# model_gradients = tape.gradient(model_loss, samples)\n",
    "# print(model_gradients)\n",
    "# # Update model\n",
    "# # optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # tf.compat.v1.enable_eager_execution()\n",
    "# samples = tf.convert_to_tensor(layer_output, np.float32)\n",
    "# # inp = tf.Variable(layer_output, dtype=tf.float32)\n",
    "# # labels = tf.convert_to_tensor(y_test[0:10], np.float32)\n",
    "# labels = y_test[0:10]\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(samples)\n",
    "# # def fun(sample):\n",
    "#     logits = grad_model(samples)[0]\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# # grads = tape.gradient(loss, inp)\n",
    "# # tf.test.compute_gradient(fun,[samples])\n",
    "# grads = K.gradients(loss,samples)\n",
    "    \n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from cleverhans.compat import softmax_cross_entropy_with_logits\n",
    "# from cleverhans.utils_keras import KerasModelWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wrapper = KerasModelWrapper( model )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample_ind = 100 # chosen by totaly random dice roll\n",
    "\n",
    "# # picking a test sample\n",
    "# sample = x_train[ sample_ind, : ]\n",
    "# true_label = y_train[ sample_ind, : ]\n",
    "\n",
    "# sample = x_train[ 0:10 ]\n",
    "# true_label = y_train[ 0:10 ]\n",
    "\n",
    "# # sample = sample.reshape(-1,80)\n",
    "# # true_label = true_label.reshape(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# myGraph = tf.Graph()\n",
    "# with myGraph.as_default():\n",
    "#     x = model.layers[ 0 ].input\n",
    "#     y = tf.placeholder( tf.float32, shape=(None, 2) )\n",
    "#     logits = model.layers[-1].input\n",
    "\n",
    "#     loss = softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "#     grads = tf.gradients(loss,x)\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# session.run(init)\n",
    "# output = session.run(grads, feed_dict={x: sample, y : true_label})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grad_model = Model(inputs=imdb_clf.inputs,outputs=imdb_clf.layers[-2].output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # an input layer to feed labels\n",
    "# y_true = Input(shape=(2,))\n",
    "# # compute loss based on model's output and true labels\n",
    "# ce = K.categorical_crossentropy(y_true, imdb_clf.output)\n",
    "# # compute gradient of loss with respect to inputs\n",
    "# grad_ce = K.gradients(ce, imdb_clf.inputs)\n",
    "# # create a function to be able to run this computation graph\n",
    "# func = K.function(imdb_clf.inputs + [y_true], grad_ce)\n",
    "\n",
    "# # usage\n",
    "# output = func([sample, real_y])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample = x_test[0:5]\n",
    "# real_y = y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output_layer = imdb_clf.outputs\n",
    "# all_layers = [layer.output for layer in imdb_clf.layers]\n",
    "# new_model = Model(inputs=imdb_clf.inputs, outputs=all_layers)\n",
    "\n",
    "# sample = tf.convert_to_tensor(x_test[0:5], tf.float32)\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "# # inputs = tf.ones((1, 299, 299, 3))\n",
    "# with tf.GradientTape() as tape:\n",
    "#     output_of_all_layers = new_model(samples)\n",
    "#     preds = output_layer[-1]  # last layer is output layer\n",
    "#     print(preds)\n",
    "#     # take gradients of last layer with respect to all layers in the model\n",
    "#     grads = tape.gradient(preds, output_of_all_layers)\n",
    "#     # note: grads[-1] should be all 1, since it it d(output)/d(output)\n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = tf.convert_to_tensor(layer_output[0:5].reshape(-1,80,128), tf.float32)\n",
    "real_y = y_test[0:5]\n",
    "\n",
    "samples = tf.Variable(sample, dtype=tf.float32)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.reset()\n",
    "    tape.watch(samples)\n",
    "#     samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "    print(\"step1\")\n",
    "    # Make prediction\n",
    "    pred_y = embed_model(samples)\n",
    "    print(\"step2\")\n",
    "    # Calculate loss\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    print(\"step3\", model_loss)\n",
    "    print()\n",
    "    #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "\n",
    "# print(tape.watched_variables())\n",
    "# Calculate gradients\n",
    "model_gradients = tape.batch_jacobian(pred_y, samples).numpy()\n",
    "print(model_gradients.reshape(-1,2,80,128).shape)\n",
    "# Update model\n",
    "# optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model_gradients.reshape(-1,2,80,128).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = imdb_classifier.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_logits = K.function([imdb_clf.layers[0].input],[imdb_clf.layers[-1].input])\n",
    "logits = get_logits(x_test[0:5])[0]\n",
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grad_func = K.function([imdb_classifier.layers[1].input], [imdb_classifier.layers[-2].output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_test[0:5], logits=logits)\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# samples = x_test[0:5]\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(samples, dtype=tf.float32)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(samples)\n",
    "#     print(\"step1\")\n",
    "#     # Make prediction\n",
    "#     pred_y = grad_model(samples)\n",
    "#     print(\"step2\")\n",
    "#     # Calculate loss\n",
    "#     model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     print(\"step3\")\n",
    "#     print(model_loss)\n",
    "#     #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    \n",
    "# # Calculate gradients\n",
    "# model_gradients = tape.gradient(model_loss, samples)\n",
    "# print(model_gradients)\n",
    "# # Update model\n",
    "# # optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # tf.compat.v1.enable_eager_execution()\n",
    "# samples = tf.convert_to_tensor(layer_output, np.float32)\n",
    "# # inp = tf.Variable(layer_output, dtype=tf.float32)\n",
    "# # labels = tf.convert_to_tensor(y_test[0:10], np.float32)\n",
    "# labels = y_test[0:10]\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(samples)\n",
    "# # def fun(sample):\n",
    "#     logits = grad_model(samples)[0]\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# # grads = tape.gradient(loss, inp)\n",
    "# # tf.test.compute_gradient(fun,[samples])\n",
    "# grads = K.gradients(loss,samples)\n",
    "    \n",
    "# print(grads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from cleverhans.compat import softmax_cross_entropy_with_logits\n",
    "# from cleverhans.utils_keras import KerasModelWrapper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wrapper = KerasModelWrapper( model )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample_ind = 100 # chosen by totaly random dice roll\n",
    "\n",
    "# # picking a test sample\n",
    "# sample = x_train[ sample_ind, : ]\n",
    "# true_label = y_train[ sample_ind, : ]\n",
    "\n",
    "# sample = x_train[ 0:10 ]\n",
    "# true_label = y_train[ 0:10 ]\n",
    "\n",
    "# # sample = sample.reshape(-1,80)\n",
    "# # true_label = true_label.reshape(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# myGraph = tf.Graph()\n",
    "# with myGraph.as_default():\n",
    "#     x = model.layers[ 0 ].input\n",
    "#     y = tf.placeholder( tf.float32, shape=(None, 2) )\n",
    "#     logits = model.layers[-1].input\n",
    "\n",
    "#     loss = softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "#     grads = tf.gradients(loss,x)\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# session.run(init)\n",
    "# output = session.run(grads, feed_dict={x: sample, y : true_label})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.layers[1].set_weights(imdb_clf.layers[2].get_weights())\n",
    "embed_model.layers[2].set_weights(imdb_clf.layers[3].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_model = Model(inputs=imdb_clf.inputs,outputs=imdb_clf.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an input layer to feed labels\n",
    "# y_true = Input(shape=(2,))\n",
    "# # compute loss based on model's output and true labels\n",
    "# ce = K.categorical_crossentropy(y_true, imdb_clf.output)\n",
    "# # compute gradient of loss with respect to inputs\n",
    "# grad_ce = K.gradients(ce, imdb_clf.inputs)\n",
    "# # create a function to be able to run this computation graph\n",
    "# func = K.function(imdb_clf.inputs + [y_true], grad_ce)\n",
    "\n",
    "# # usage\n",
    "# output = func([sample, real_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = x_test[0:5]\n",
    "# real_y = y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_layer = imdb_clf.outputs\n",
    "# all_layers = [layer.output for layer in imdb_clf.layers]\n",
    "# new_model = Model(inputs=imdb_clf.inputs, outputs=all_layers)\n",
    "\n",
    "# sample = tf.convert_to_tensor(x_test[0:5], tf.float32)\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "# # inputs = tf.ones((1, 299, 299, 3))\n",
    "# with tf.GradientTape() as tape:\n",
    "#     output_of_all_layers = new_model(samples)\n",
    "#     preds = output_layer[-1]  # last layer is output layer\n",
    "#     print(preds)\n",
    "#     # take gradients of last layer with respect to all layers in the model\n",
    "#     grads = tape.gradient(preds, output_of_all_layers)\n",
    "#     # note: grads[-1] should be all 1, since it it d(output)/d(output)\n",
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1\n",
      "step2\n",
      "step3 tf.Tensor([0.24223822 0.03356474 0.31229496 0.49673867 0.05070358], shape=(5,), dtype=float32)\n",
      "\n",
      "(5, 2, 80, 128)\n"
     ]
    }
   ],
   "source": [
    "sample = tf.convert_to_tensor(layer_output[0:5].reshape(-1,80,128), tf.float32)\n",
    "real_y = y_test[0:5]\n",
    "\n",
    "samples = tf.Variable(sample, dtype=tf.float32)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "#     tape.reset()\n",
    "    tape.watch(samples)\n",
    "#     samples = tf.Variable(sample, dtype=tf.float32)\n",
    "\n",
    "    print(\"step1\")\n",
    "    # Make prediction\n",
    "    pred_y = embed_model(samples)\n",
    "    print(\"step2\")\n",
    "    # Calculate loss\n",
    "    model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    print(\"step3\", model_loss)\n",
    "    print()\n",
    "    #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "\n",
    "# print(tape.watched_variables())\n",
    "# Calculate gradients\n",
    "model_gradients = tape.batch_jacobian(pred_y, samples).numpy()\n",
    "print(model_gradients.reshape(-1,2,80,128).shape)\n",
    "# Update model\n",
    "# optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 80, 128)\n"
     ]
    }
   ],
   "source": [
    "print(model_gradients.reshape(-1,2,80,128).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 14s 566us/step\n",
      "Test score: 0.3708902388572693\n",
      "Test accuracy: 0.8351200222969055\n"
     ]
    }
   ],
   "source": [
    "score, acc = imdb_classifier.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5548203 , -0.61283696],\n",
       "       [ 1.2027019 , -1.2958295 ],\n",
       "       [ 0.11605286, -0.14667611],\n",
       "       [ 0.62695706, -0.6806632 ],\n",
       "       [ 1.1918951 , -1.2904471 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits = K.function([imdb_clf.layers[0].input],[imdb_clf.layers[-1].input])\n",
    "logits = get_logits(x_test[0:5])[0]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_func = K.function([imdb_classifier.layers[1].input], [imdb_classifier.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([0.27086258, 0.07900123, 0.5703863 , 0.23938139, 0.08024024],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_test[0:5], logits=logits)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = x_test[0:5]\n",
    "# real_y = y_test[0:5]\n",
    "\n",
    "# samples = tf.Variable(samples, dtype=tf.float32)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     tape.watch(samples)\n",
    "#     print(\"step1\")\n",
    "#     # Make prediction\n",
    "#     pred_y = grad_model(samples)\n",
    "#     print(\"step2\")\n",
    "#     # Calculate loss\n",
    "#     model_loss = tf.nn.softmax_cross_entropy_with_logits(labels=real_y, logits=pred_y)\n",
    "#     print(\"step3\")\n",
    "#     print(model_loss)\n",
    "#     #         model_loss = tf.keras.losses.categorical_crossentropy(real_y, pred_y)\n",
    "    \n",
    "# # Calculate gradients\n",
    "# model_gradients = tape.gradient(model_loss, samples)\n",
    "# print(model_gradients)\n",
    "# # Update model\n",
    "# # optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    }
   ],
   "source": [
    "# # tf.compat.v1.enable_eager_execution()\n",
    "# samples = tf.convert_to_tensor(layer_output, np.float32)\n",
    "# # inp = tf.Variable(layer_output, dtype=tf.float32)\n",
    "# # labels = tf.convert_to_tensor(y_test[0:10], np.float32)\n",
    "# labels = y_test[0:10]\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(samples)\n",
    "# # def fun(sample):\n",
    "#     logits = grad_model(samples)[0]\n",
    "#     loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "# # grads = tape.gradient(loss, inp)\n",
    "# # tf.test.compute_gradient(fun,[samples])\n",
    "# grads = K.gradients(loss,samples)\n",
    "    \n",
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from cleverhans.compat import softmax_cross_entropy_with_logits\n",
    "# from cleverhans.utils_keras import KerasModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wrapper = KerasModelWrapper( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sample_ind = 100 # chosen by totaly random dice roll\n",
    "\n",
    "# # picking a test sample\n",
    "# sample = x_train[ sample_ind, : ]\n",
    "# true_label = y_train[ sample_ind, : ]\n",
    "\n",
    "# sample = x_train[ 0:10 ]\n",
    "# true_label = y_train[ 0:10 ]\n",
    "\n",
    "# # sample = sample.reshape(-1,80)\n",
    "# # true_label = true_label.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myGraph = tf.Graph()\n",
    "# with myGraph.as_default():\n",
    "#     x = model.layers[ 0 ].input\n",
    "#     y = tf.placeholder( tf.float32, shape=(None, 2) )\n",
    "#     logits = model.layers[-1].input\n",
    "\n",
    "#     loss = softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "\n",
    "#     grads = tf.gradients(loss,x)\n",
    "\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# session.run(init)\n",
    "# output = session.run(grads, feed_dict={x: sample, y : true_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}